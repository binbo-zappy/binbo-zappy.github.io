<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>class3-week3-无监督学习-强化学习-吴恩达</title>
    <link href="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    <url>/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013185527334.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013185513926.png"></p><ul><li>状态、动作、奖励和下一个状态（s，a，R（s），s‘)</li></ul><h3 id="1-回报"><a href="#1-回报" class="headerlink" title="1. 回报"></a>1. 回报</h3><ul><li>折扣因子</li></ul><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013191147456.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013191515723.png"></p><h3 id="2-决策"><a href="#2-决策" class="headerlink" title="2. 决策"></a>2. 决策</h3><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013191936751.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013192130503.png"></p><ul><li><p>马尔可夫决策过程 MDP</p></li><li><p>在马尔可夫决策过程中，未来只取决于你现在所处的位置，而不取决于你是如何到达这里的。</p></li></ul><h3 id="3-状态-动作价值函数"><a href="#3-状态-动作价值函数" class="headerlink" title="3. 状态-动作价值函数"></a>3. 状态-动作价值函数</h3><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013214059896.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013214515098.png"></p><h4 id="3-1-贝尔曼方程"><a href="#3-1-贝尔曼方程" class="headerlink" title="3.1 贝尔曼方程"></a>3.1 贝尔曼方程</h4><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013220256063.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013221245911.png"></p><ul><li>如果你从状态s 开始，你将采取行动a，然后在此之后采取最佳行动，那么你将随着时间的推移看到一些奖励序列。</li></ul><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013221652384.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013221916868.png"></p><h3 id="4-随机强化学习"><a href="#4-随机强化学习" class="headerlink" title="4. 随机强化学习"></a>4. 随机强化学习</h3><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013222607911.png"></p><h3 id="5-连续状态"><a href="#5-连续状态" class="headerlink" title="5. 连续状态"></a>5. 连续状态</h3><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013222846102.png"></p><ul><li>连续马尔可夫 MTP</li></ul><h3 id="6-学习状态值函数"><a href="#6-学习状态值函数" class="headerlink" title="6. 学习状态值函数"></a>6. 学习状态值函数</h3><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013223922575.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013224639866.png"></p><ul><li>不知道Q，随机猜测</li></ul><h3 id="7-DQN"><a href="#7-DQN" class="headerlink" title="7. DQN"></a>7. DQN</h3><ul><li>意思是神经网络里的参数随机初始化，然后(s’,a’)输入，得到maxQ的预测</li></ul><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013225256166.png"></p><ul><li>交给神经网络训练的参数y中一部分是随机初始化神经网络生成的，但还有一部分是包含了当前状态的信息的，所以当训练次数增多后，外部的输入信息会逐步冲刷掉初始化的随机信息，给出真正的Q函数估计。</li></ul><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013225733542.png"></p><h4 id="7-1-贪婪算法"><a href="#7-1-贪婪算法" class="headerlink" title="7.1 贪婪算法"></a>7.1 贪婪算法</h4><ul><li>使用高ε开始，逐步降低直到0.01</li></ul><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013230506858.png"></p><h4 id="7-2-小批量和软更新"><a href="#7-2-小批量和软更新" class="headerlink" title="7.2 小批量和软更新"></a>7.2 小批量和软更新</h4><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013230853479.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013231149426.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013231255361.png"></p><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013231653864.png"></p><h3 id="8-局限性"><a href="#8-局限性" class="headerlink" title="8. 局限性"></a>8. 局限性</h3><p><img src="/2024/11/14/ML/class3-week3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013232030585.png"></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>ML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.2 无监督学习-推荐系统-吴恩达</title>
    <link href="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    <url>/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><h3 id="1-使用每个特征数据"><a href="#1-使用每个特征数据" class="headerlink" title="1. 使用每个特征数据"></a>1. 使用每个特征数据</h3><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013091417801.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013092525171.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013092709205.png"></p><h3 id="2-协同过滤算法"><a href="#2-协同过滤算法" class="headerlink" title="2. 协同过滤算法"></a>2. 协同过滤算法</h3><ul><li>假设已经有了w和b，猜测特征x</li></ul><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013093541589.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013094256436.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013094536963.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013095022269.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013095116749.png"></p><ul><li><p>这种协同过滤是从多个用户收集数据，用户之间的这种协作可帮助您预测未来甚至其他用户的评级。</p></li><li><p>推荐系统的一个非常常见的用例是当您有二进制标签时，例如用户喜欢、喜欢或与项目交互的标签。</p></li></ul><h3 id="3-二进制标签"><a href="#3-二进制标签" class="headerlink" title="3.二进制标签"></a>3.二进制标签</h3><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013095506383.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013095650568.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013095901608.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013100113430.png"></p><ul><li>分类不用正则化</li></ul><h3 id="4-均值归一化"><a href="#4-均值归一化" class="headerlink" title="4. 均值归一化"></a>4. 均值归一化</h3><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013143727458.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013144130791.png"></p><ul><li>对未知用户，预测评分为均值</li></ul><h3 id="5-协同过滤Tensorflow实现"><a href="#5-协同过滤Tensorflow实现" class="headerlink" title="5. 协同过滤Tensorflow实现"></a>5. 协同过滤Tensorflow实现</h3><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013144725579.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013144803015.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013145539351.png"></p><h3 id="6-寻找相关特征"><a href="#6-寻找相关特征" class="headerlink" title="6. 寻找相关特征"></a>6. 寻找相关特征</h3><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013145951931.png"></p><ul><li>协同过滤的局限性</li></ul><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013150021292.png"></p><ul><li><p>冷启动问题</p></li><li><p>边缘信息</p></li></ul><h3 id="7-基于内容的过滤算法"><a href="#7-基于内容的过滤算法" class="headerlink" title="7. 基于内容的过滤算法"></a>7. 基于内容的过滤算法</h3><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013150639264.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013150936617.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013151110932.png"></p><ul><li>如何计算V</li></ul><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013151407807.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013151905392.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013152534237.png"></p><h3 id="8-从大型目录中推荐"><a href="#8-从大型目录中推荐" class="headerlink" title="8. 从大型目录中推荐"></a>8. 从大型目录中推荐</h3><ul><li>两个步骤：检索和排名</li></ul><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013152823434.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013153036731.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013153343967.png"></p><h3 id="9-基于内容的Tensorflow实现"><a href="#9-基于内容的Tensorflow实现" class="headerlink" title="9. 基于内容的Tensorflow实现"></a>9. 基于内容的Tensorflow实现</h3><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013154612258.png"></p><h3 id="10-降低特征数量"><a href="#10-降低特征数量" class="headerlink" title="10. 降低特征数量"></a>10. 降低特征数量</h3><ul><li>PCA主成分分析法，特征降为二维或者三维，便于可视化</li></ul><h4 id="10-1-PCA算法"><a href="#10-1-PCA算法" class="headerlink" title="10.1 PCA算法"></a>10.1 PCA算法</h4><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013170727751.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013171217861.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013171447596.png"></p><ul><li><p>当使用线性回归来预测目标输出Y并且PCA试图获取大量特征并平等对待它们并减少很好地表示数据所需的轴数</p></li><li><p>因此，如果您尝试预测y的值，则应使用线性回归;如果您尝试减少数据集中的特征数量，例如将其可视化，则应使用PCA。</p></li></ul><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013172253829.png"></p><ul><li>每个特征的方差贡献率</li></ul><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013172530272.png"></p><p><img src="/2024/11/14/ML/class3-week2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013172923497.png"></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>ML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.1 无监督学习-无监督学习-吴恩达</title>
    <link href="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    <url>/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><h3 id="1-聚类"><a href="#1-聚类" class="headerlink" title="1. 聚类"></a>1. 聚类</h3><h4 id="1-1-k-means"><a href="#1-1-k-means" class="headerlink" title="1.1 k-means"></a>1.1 k-means</h4><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012125528206.png"></p><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012125536447.png"></p><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012130426099.png"></p><ul><li>如果一个集群训练样本为零，可以消除该集群，最终得到k-1；另一种方法是重新初始化该集群质心</li></ul><h4 id="1-2-优化目标"><a href="#1-2-优化目标" class="headerlink" title="1.2 优化目标"></a>1.2 优化目标</h4><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012145049473.png"></p><h4 id="1-3-初始化"><a href="#1-3-初始化" class="headerlink" title="1.3 初始化"></a>1.3 初始化</h4><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012150757199.png"></p><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012151542672.png"></p><h4 id="1-4-选择聚类数量"><a href="#1-4-选择聚类数量" class="headerlink" title="1.4 选择聚类数量"></a>1.4 选择聚类数量</h4><ul><li>肘法</li></ul><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012151925053.png"></p><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012152146445.png"></p><h3 id="2-异常检测"><a href="#2-异常检测" class="headerlink" title="2. 异常检测"></a>2. 异常检测</h3><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012152706177.png"></p><h4 id="2-1-密度估计"><a href="#2-1-密度估计" class="headerlink" title="2.1 密度估计"></a>2.1 密度估计</h4><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012152845235.png"></p><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012153346742.png"></p><h4 id="2-2-高斯正态分布"><a href="#2-2-高斯正态分布" class="headerlink" title="2.2 高斯正态分布"></a>2.2 高斯正态分布</h4><ul><li>最大似然估计</li></ul><h4 id="2-3-异常检测算法"><a href="#2-3-异常检测算法" class="headerlink" title="2.3 异常检测算法"></a>2.3 异常检测算法</h4><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012154133115.png"></p><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012154923207.png"></p><h4 id="2-4-开发与评估异常检测系统"><a href="#2-4-开发与评估异常检测系统" class="headerlink" title="2.4 开发与评估异常检测系统."></a>2.4 开发与评估异常检测系统.</h4><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012215712447.png"></p><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012220215931.png"></p><ul><li><p>这种替代方案的缺点是，在调整算法后，您没有公平的方法来判断它在未来示例中的实际效果如何，因为您没有测试集。</p></li><li><p>当你的数据集很小的时候，特别是当你有异常的数量时，你的数据集很小，这可能是你最好的选择。</p></li></ul><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013082659472.png"></p><h4 id="2-5-异常检测vs监督学习"><a href="#2-5-异常检测vs监督学习" class="headerlink" title="2.5 异常检测vs监督学习"></a>2.5 异常检测vs监督学习</h4><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013084827450.png"></p><ul><li>例子</li></ul><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013084924028.png"></p><h4 id="2-6-选择用什么特征"><a href="#2-6-选择用什么特征" class="headerlink" title="2.6 选择用什么特征"></a>2.6 选择用什么特征</h4><ul><li>特征改变为高斯分布</li></ul><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013085546244.png"></p><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013090207839.png"></p><p><img src="/2024/11/14/ML/class3-week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231013090413780.png"></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>ML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.4 深度学习-决策树-吴恩达</title>
    <link href="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    <url>/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012075401986.png"></p><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012075410387.png"></p><ul><li><p>根节点 决策节点 叶节点</p></li><li><p>决策树学习算法的工作是，从所有可能的决策树中，尝试选择一个希望在训练集上表现良好的树，然后理想地泛化到新数据，例如交叉验证和测试集</p></li></ul><h3 id="1-学习过程"><a href="#1-学习过程" class="headerlink" title="1. 学习过程"></a>1. 学习过程</h3><ul><li>决策树学习的第一步是，我们必须决定在根节点使用什么特征。</li></ul><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012081611286.png"></p><ul><li><p>决策 2：何时停止分裂？</p><ul><li>当一个节点完全是一类时</li><li>当分裂一个节点会导致树超过最大深度时</li><li>当纯度分数的提高低于一个阈值时</li><li>当节点中的样本数量低于一个阈值时</li></ul></li><li><p>您可能想要限制决策树深度的一个原因是确保我们的树不会变得太大和笨重，其次，通过保持树小，它不太容易过度拟合。</p></li></ul><h3 id="2-纯度"><a href="#2-纯度" class="headerlink" title="2. 纯度"></a>2. 纯度</h3><h4 id="2-1-熵-entropy"><a href="#2-1-熵-entropy" class="headerlink" title="2.1 熵 entropy"></a>2.1 熵 entropy</h4><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012082638836.png"></p><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012082823450.png"></p><ul><li><p>选择拆分信息增益</p></li><li><p>减少熵</p></li><li><p>信息增益 分之前的熵减去分后熵的加权平均</p></li><li><p>停止标准，每次信息增益如果太小停止分类</p></li></ul><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012083632353.png"></p><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012083921344.png"></p><h4 id="2-2-整合"><a href="#2-2-整合" class="headerlink" title="2.2 整合"></a>2.2 整合</h4><ul><li>从根节点开始，包含所有样本</li><li>计算所有可能特征的信息增益，并选择信息增益最高的特征</li><li>根据选定的特征划分数据集，并创建树的左分支和右分支</li><li>持续重复分裂过程，直到满足停止条件：<ul><li>当一个节点完全是一类时</li><li>当分裂一个节点会导致树超过最大深度时</li><li>额外分裂的信息增益小于阈值时</li><li>当节点中的样本数量低于阈值时</li></ul></li></ul><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012084601880.png"></p><ul><li>递归分类</li></ul><h3 id="3-独热编码-one-hot"><a href="#3-独热编码-one-hot" class="headerlink" title="3. 独热编码 one-hot"></a>3. 独热编码 one-hot</h3><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012085239987.png"></p><ul><li><p>通过one-hot编码，您可以让决策树处理可以采用两个以上离散值的特征，您还可以将其应用于新网络或线性回归或逻辑回归训练。</p></li><li><p>连续值</p></li><li><p>尝试不同的阈值，计算纯度</p></li></ul><h3 id="4-回归树"><a href="#4-回归树" class="headerlink" title="4. 回归树"></a>4. 回归树</h3><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012090404205.png"></p><ul><li>尝试减少方差</li></ul><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012091154958.png"></p><h3 id="5-使用多个决策树"><a href="#5-使用多个决策树" class="headerlink" title="5. 使用多个决策树"></a>5. 使用多个决策树</h3><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012092643280.png"></p><h4 id="5-1-有放回的采样"><a href="#5-1-有放回的采样" class="headerlink" title="5.1 有放回的采样"></a>5.1 有放回的采样</h4><ul><li><p>构建新的数据集</p></li><li><p>随机森林算法 Random Forest Algorithm</p></li></ul><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012093230276.png"></p><ul><li><p>随机化特征选择</p><ul><li>在每个节点选择用于分裂的特征时，如果有n个特征可用，随机选择一个包含k个特征的子集（k &lt; n），并允许算法仅从这个特征子集中进行选择。</li></ul></li><li><p>k的选择（根号n，log2(n)）</p></li></ul><h4 id="5-2-XGBoost-extreme-Gradient-Boosting"><a href="#5-2-XGBoost-extreme-Gradient-Boosting" class="headerlink" title="5.2 XGBoost (extreme Gradient Boosting)"></a>5.2 XGBoost (extreme Gradient Boosting)</h4><ul><li>提升树的开源实现</li><li>快速高效的实现</li><li>默认分裂标准和停止分裂标准的好选择</li><li>内置正则化以防止过拟合</li><li>机器学习竞赛中极具竞争力的算法（例如：Kaggle竞赛）</li></ul><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012094738491.png"></p><h3 id="6-何时使用决策树"><a href="#6-何时使用决策树" class="headerlink" title="6. 何时使用决策树"></a>6. 何时使用决策树</h3><ul><li>表格数据</li></ul><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012095356821.png"></p><p><img src="/2024/11/14/ML/class2-week4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231012095814528.png"></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>ML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.3 深度学习-在机器学习项目中下一步该做什么-吴恩达</title>
    <link href="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    <url>/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="在机器学习项目中下一步该做什么"><a href="#在机器学习项目中下一步该做什么" class="headerlink" title="在机器学习项目中下一步该做什么"></a>在机器学习项目中下一步该做什么</h2><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011083027133.png"></p><h3 id="1-模型评估"><a href="#1-模型评估" class="headerlink" title="1. 模型评估"></a>1. 模型评估</h3><p>训练集分为两个子集</p><ul><li><p>training set</p></li><li><p>test set</p></li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011083802084.png"></p><p>不包含正则化项</p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011084012544.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011084442975.png" alt="image-20231011084442975"></p><p>就是相比于之前计算在测试集和训练集上误差的那两个公式，我们更常用模型分类错误的次数除以总的预测次数来表示误差</p><p>就是对于逻辑回归来说，可以通过计算误判占比的方法来代表成本函数，比如test set中误判的占比是10％，那么J test就是0.1</p><h4 id="1-1-分为三个子集，训练集、交叉验证集、测试集"><a href="#1-1-分为三个子集，训练集、交叉验证集、测试集" class="headerlink" title="1.1 分为三个子集，训练集、交叉验证集、测试集"></a>1.1 分为三个子集，训练集、交叉验证集、测试集</h4><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011091646019.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011091757791.png"></p><ul><li><p>选择最小的交叉验证集误差对应的模型</p></li><li><p>用测试集来评估泛化误差</p></li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011092256493.png"></p><h4 id="1-2-偏差和方差"><a href="#1-2-偏差和方差" class="headerlink" title="1.2 偏差和方差"></a>1.2 偏差和方差</h4><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011093159868.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011093401564.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011093433520.png"></p><ul><li>正则化如何影响偏差和方差，从而影响算法的性能</li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011093859951.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011094124259.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011094256539.png"></p><h4 id="1-3-指定一个用于性能评估的基准"><a href="#1-3-指定一个用于性能评估的基准" class="headerlink" title="1.3 指定一个用于性能评估的基准"></a>1.3 指定一个用于性能评估的基准</h4><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011103735186.png"></p><p>竞争算法</p><h4 id="1-4-学习曲线"><a href="#1-4-学习曲线" class="headerlink" title="1.4 学习曲线"></a>1.4 学习曲线</h4><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011104512509.png"></p><ul><li>训练集变大，误差增大</li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011104852894.png"></p><ul><li>这给出了这个结论，也许有点令人惊讶，如果学习算法具有高偏差，获得更多的训练数据本身就没有那么大的希望。</li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011105426342.png"></p><ul><li>所以在这种情况下，可能仅仅通过增加训练集的大小来降低交叉验证误差并让你的算法表现得越来越好，这与高偏差情况不同，在这种情况下你唯一要做的就是获得更多的训练数据，实际上并不能帮助您了解算法性能。</li></ul><h4 id="1-5-如何改进"><a href="#1-5-如何改进" class="headerlink" title="1.5 如何改进"></a>1.5 如何改进</h4><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011110222504.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011110708303.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011111707005.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011112028987.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011112250065.png"></p><h3 id="2-机器学习开发的迭代"><a href="#2-机器学习开发的迭代" class="headerlink" title="2. 机器学习开发的迭代"></a>2. 机器学习开发的迭代</h3><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011112605831.png"></p><ul><li>垃圾邮件分类</li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011112929003.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011113910756.png"></p><h4 id="2-1-误差分析"><a href="#2-1-误差分析" class="headerlink" title="2.1 误差分析"></a>2.1 误差分析</h4><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011114442995.png"></p><h4 id="2-2-数据增强"><a href="#2-2-数据增强" class="headerlink" title="2.2 数据增强"></a>2.2 数据增强</h4><ul><li>旋转图像扭曲放大缩小</li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011115040740.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011115116926.png"></p><ul><li>音频增强</li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011115211067.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011115324412.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011131113956.png"></p><h4 id="2-3-迁移学习-Transfer-learning"><a href="#2-3-迁移学习-Transfer-learning" class="headerlink" title="2.3 迁移学习 Transfer  learning"></a>2.3 迁移学习 Transfer  learning</h4><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011163140324.png"></p><ul><li><p>数据集小选择1，数据集大选择2</p></li><li><p>先在大的数据集训练（监督与训练），再在小的训练称为微调</p></li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011163230072.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011163351440.png"></p><h4 id="2-4-机器学习项目全周期"><a href="#2-4-机器学习项目全周期" class="headerlink" title="2.4 机器学习项目全周期"></a>2.4 机器学习项目全周期</h4><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011163931188.png"></p><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011164414049.png"></p><h4 id="2-5-倾斜数据集的误差指标"><a href="#2-5-倾斜数据集的误差指标" class="headerlink" title="2.5 倾斜数据集的误差指标"></a>2.5 倾斜数据集的误差指标</h4><ul><li>精确度和召回率</li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011165618174.png"></p><ul><li>精度和召回率</li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011170439331.png"></p><ul><li>F1score 更强调P和R中较低的那个 调和平均值</li></ul><p><img src="/2024/11/14/ML/class2-week3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231011170740787.png"></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>ML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.2 深度学习-Tenserflow实现-吴恩达</title>
    <link href="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    <url>/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="Tenserflow实现"><a href="#Tenserflow实现" class="headerlink" title="Tenserflow实现"></a>Tenserflow实现</h2><h3 id="1-模型训练步骤"><a href="#1-模型训练步骤" class="headerlink" title="1. 模型训练步骤"></a>1. 模型训练步骤</h3><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010125603129.png" alt="模型训练步骤"></p><h4 id="1-1-Epochs-and-batches"><a href="#1-1-Epochs-and-batches" class="headerlink" title="1.1 Epochs and batches"></a>1.1 Epochs and batches</h4><p>在上述的 <code>compile</code> 语句中，<code>epochs</code> 的数量被设置为10。这指定了整个数据集在训练过程中应该被应用10次。在训练期间，你会看到描述训练进度的输出，看起来像这样：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Epoch</span> <span class="hljs-number">1</span>/<span class="hljs-number">10</span><br><span class="hljs-attribute">6250</span>/<span class="hljs-number">6250</span><span class="hljs-meta"> [==============================] - 6s 910us/step - loss: 0.1782</span><br></code></pre></td></tr></table></figure><p>第一行 <code>Epoch 1/10</code> 描述了模型当前正在运行的是哪个训练周期。为了提高效率，训练数据集被分成了“批次”。在Tensorflow中，默认的批次大小是32。我们的扩展数据集中有200000个样本，或者6250个批次。第二行的符号 <code>6250/6250 [====</code> 描述了已经执行了哪个批次。</p><h4 id="1-2-创建模型"><a href="#1-2-创建模型" class="headerlink" title="1.2 创建模型"></a>1.2 创建模型</h4><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010125701208.png"></p><h4 id="1-3-交叉熵损失函数"><a href="#1-3-交叉熵损失函数" class="headerlink" title="1.3 交叉熵损失函数"></a>1.3 交叉熵损失函数</h4><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010130715345.png"></p><ul><li>回归和分类使用不同的损失代价函数</li></ul><h4 id="1-4-梯度下降"><a href="#1-4-梯度下降" class="headerlink" title="1.4 梯度下降"></a>1.4 梯度下降</h4><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010130841602.png"></p><p>多层感知器：多层神经网路</p><h3 id="2-激活函数"><a href="#2-激活函数" class="headerlink" title="2. 激活函数"></a>2. 激活函数</h3><h4 id="2-1-ReLU-Activation"><a href="#2-1-ReLU-Activation" class="headerlink" title="2.1 ReLU Activation"></a>2.1 ReLU Activation</h4><p>This week, a new activation was introduced, the Rectified Linear Unit (ReLU).</p><p>𝑎&#x3D;𝑚𝑎𝑥(0,𝑧) </p><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010204558837.png" alt="ReLU"></p><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010204715345.png"></p><ul><li><p>二分类问题，sigmoid激活函数是最自然的选择，输出层使用</p></li><li><p>回归模型，输出有正有负，预测明天的股票价格，输出层使用线性激活函数</p></li><li><p>回归模型，输出为非负，预测房屋价格，非负，输出层选择ReLU函数</p></li><li><p>隐藏层选择Relu函数，ReLU计算速度更快，效率高，但事实证明更重要的第二个原因是ReLU函数仅在图形的一部分变平;左边这里<br>是完全平坦的，而sigmoid激活函数，它在两个地方变得平坦。梯度下降就会很慢，减慢学习速度</p></li><li><p>为什么要使用激活函数？</p><ul><li>若所有层都是用线性激活函数，那就变成了线性回归</li></ul></li></ul><h3 id="3-多分类问题"><a href="#3-多分类问题" class="headerlink" title="3. 多分类问题"></a>3. 多分类问题</h3><h4 id="3-1-Softmax函数"><a href="#3-1-Softmax函数" class="headerlink" title="3.1 Softmax函数"></a>3.1 Softmax函数</h4><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010212144511.png"></p><h4 id="3-2-代价函数"><a href="#3-2-代价函数" class="headerlink" title="3.2 代价函数"></a>3.2 代价函数</h4><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010212557352.png"></p><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010220105018.png"></p><h4 id="3-3-Tenserflow-实现"><a href="#3-3-Tenserflow-实现" class="headerlink" title="3.3 Tenserflow 实现"></a>3.3 Tenserflow 实现</h4><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010220327241.png"></p><h4 id="3-4-改进实现"><a href="#3-4-改进实现" class="headerlink" title="3.4 改进实现"></a>3.4 改进实现</h4><p>避免计算过程中出现过大或者过小值造成计算错误，改进方法在计算过程中进行了重新排列</p><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010221115720.png"></p><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010221418604.png"></p><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010221520301.png"></p><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010221537117.png"></p><h3 id="4-多标签分类"><a href="#4-多标签分类" class="headerlink" title="4. 多标签分类"></a>4. 多标签分类</h3><p>一个神经网络同时检测多个目标</p><h3 id="5-更快的训练方法"><a href="#5-更快的训练方法" class="headerlink" title="5. 更快的训练方法"></a>5. 更快的训练方法</h3><h4 id="5-1-Adam算法"><a href="#5-1-Adam算法" class="headerlink" title="5.1 Adam算法"></a>5.1 Adam算法</h4><p>自动调节α</p><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010231015869.png"></p><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010231157191.png"></p><h3 id="6-其他的网络层"><a href="#6-其他的网络层" class="headerlink" title="6. 其他的网络层"></a>6. 其他的网络层</h3><p>密集层</p><p>卷积层</p><ul><li><p>更快的计算</p></li><li><p>需要更少的训练数据，不太会过拟合</p></li></ul><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010231710233.png"></p><h3 id="7-计算图"><a href="#7-计算图" class="headerlink" title="7. 计算图"></a>7. 计算图</h3><p><img src="/2024/11/13/ML/class2-week2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tenserflow%E5%AE%9E%E7%8E%B0-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231010235541548.png"></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>ML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.1 深度学习-神经网络-吴恩达</title>
    <link href="/2024/11/13/ML/class2-week1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    <url>/2024/11/13/ML/class2-week1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><ul><li><p>输入层 输入为特征向量</p></li><li><p>隐藏层</p></li><li><p>输出层</p></li></ul><p><strong>Tensorflow and Keras</strong>  </p><ul><li>TensorFlow 是由谷歌开发的一个机器学习包。2019年，谷歌将 Keras 集成到 TensorFlow 中，并发布了 TensorFlow 2.0。Keras 是由 François Chollet 独立开发的框架，它为 TensorFlow 创建了一个简单、以层为中心的接口。本课程将使用 Keras 接口。</li></ul><h3 id="前向传播算法"><a href="#前向传播算法" class="headerlink" title="前向传播算法"></a>前向传播算法</h3><p><img src="/2024/11/13/ML/class2-week1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231009222124079.png"></p><ul><li><p>从左到右向前计算，称为前向传播。</p></li><li><p>离输出层越近，隐藏层神经元越少。</p></li></ul><p><img src="/2024/11/13/ML/class2-week1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231009222647437.png"></p><ul><li><p>两个特征，第一个隐藏层有三个神经元，激活函数为sigmoid，输出为a1，第二层同理；</p></li><li><p>输入向量要写成二维矩阵形式 x &#x3D; np.array([[200,17]])  1x2矩阵。</p></li></ul><p><img src="/2024/11/13/ML/class2-week1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231009224920509.png"></p><p><img src="/2024/11/13/ML/class2-week1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%90%B4%E6%81%A9%E8%BE%BE/image-20231009224946845.png"></p><ul><li>将权重拟合到数据（反向传播）如果数据被归一化，那么这个过程将会进行得更快。</li></ul>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>ML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.3 有监督机器学习回归和分类-逻辑回归-吴恩达</title>
    <link href="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <url>/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h2 id="Week3-逻辑回归"><a href="#Week3-逻辑回归" class="headerlink" title="Week3 逻辑回归"></a>Week3 逻辑回归</h2><h3 id="1-二元分类"><a href="#1-二元分类" class="headerlink" title="1. 二元分类"></a>1. 二元分类</h3><p>只有两种可能输出的分类问题称为二元分类。</p><h3 id="2-sigmoid-函数"><a href="#2-sigmoid-函数" class="headerlink" title="2. sigmoid 函数"></a>2. sigmoid 函数</h3><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008173829773.png"></p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008173915794.png" alt="sigmoid"></p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20241113221925070.png"></p><p>输入特征，输出0到1</p><h3 id="3-决策边界"><a href="#3-决策边界" class="headerlink" title="3. 决策边界"></a>3. 决策边界</h3><p>线性or非线性</p><h3 id="4-代价函数"><a href="#4-代价函数" class="headerlink" title="4. 代价函数"></a>4. 代价函数</h3><p>平方误差成本函数不是逻辑回归的理想成本函数，常用的是对数损失函数。</p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008205513401.png" alt="squared error cost"></p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008205613800.png" alt="凸和非凸"></p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008205638121.png" alt="image-20231008205638121"></p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008205807667.png" alt="logistic loss function"></p><p>请记住，损失函数衡量的是你在一个训练样例上的表现如何，它是通过总结你随后获得的所有训练样例的损失，成本函数衡量你在整个训练集上的表现。</p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008210426343.png" alt="image-20231008210426343"></p><p>整体成本函数为凸函数，可以获得全局最小值</p><p>简化损失函数</p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008211105952.png" alt="image-20231008211105952"></p><p>代价函数</p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008211415275.png" alt="image-20231008211415275"></p><p>使用最大似然估计推导出来，是凸函数。</p><h3 id="5-梯度下降"><a href="#5-梯度下降" class="headerlink" title="5. 梯度下降"></a>5. 梯度下降</h3><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008211755913.png" alt="image-20231008211755913"></p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008211932119.png" alt="image-20231008211932119"></p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008211950319.png" alt="image-20231008211950319"></p><p>Same concepts:</p><ul><li>Monitor gradient descent(learning curve)</li><li>Vectorized implementation</li><li>Feature scaling</li></ul><h3 id="6-过拟合与欠拟合"><a href="#6-过拟合与欠拟合" class="headerlink" title="6. 过拟合与欠拟合"></a>6. 过拟合与欠拟合</h3><ul><li><p>欠拟合：高偏差 high bias  Does not fit thetraining set well</p></li><li><p>过拟合：高方差 high variance Fits the training setextremely well</p></li></ul><h3 id="7-解决过拟合"><a href="#7-解决过拟合" class="headerlink" title="7. 解决过拟合"></a>7. 解决过拟合</h3><ul><li><p>收集更多的训练样本</p></li><li><p>选择特征   select features to include&#x2F;exclude</p></li><li><p>正则化 Regularization 正则化是一种更温和地减少某些特征影响的方法，而不用像彻底消除它那样严厉。</p></li><li><p>那么正则化的作用是，它可以让你保留所有特征，但它们只是防止特征产生过大的影响，而这有时会导致过度拟合。</p></li></ul><h3 id="8-正则化-Regularization"><a href="#8-正则化-Regularization" class="headerlink" title="8. 正则化 Regularization"></a>8. 正则化 Regularization</h3><p>λ正则化参数</p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008223908932.png" alt="image-20231008223908932"></p><p>正则化线性回归</p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008224030737.png" alt="image-20231008224030737"></p><p>梯度下降</p><p><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008224109310.png" alt="image-20231008224109310"></p><p>正则化逻辑回归</p><h1 id><a href="#" class="headerlink" title></a><img src="/2024/11/13/ML/class1-week3-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20231008225551317.png" alt="image-20231008225551317"></h1>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>ML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于演员-评论家框架的层次化多智能体协同决策方法</title>
    <link href="/2024/11/12/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E5%88%86%E9%85%8D/%E5%9F%BA%E4%BA%8E%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%AE%B6%E6%A1%86%E6%9E%B6%E7%9A%84%E5%B1%82%E6%AC%A1%E5%8C%96%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%8F%E5%90%8C%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/"/>
    <url>/2024/11/12/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E5%88%86%E9%85%8D/%E5%9F%BA%E4%BA%8E%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%AE%B6%E6%A1%86%E6%9E%B6%E7%9A%84%E5%B1%82%E6%AC%A1%E5%8C%96%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%8F%E5%90%8C%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><p>这篇文章的主要内容是提出了一种基于演员-评论家（Actor-Critic，AC）框架的层次化多智能体协同决策方法，旨在解决复杂作战环境下多智能体协同决策中的任务分配不合理和决策一致性较差的问题。该方法通过将决策过程分为不同层次，并使用AC框架来实现智能体之间的信息交流和决策协同，以提高决策效率和战斗力。在高层次，顶层智能体制定任务决策，将总任务分解并分配给底层智能体。在低层次，底层智能体根据子任务进行动作决策，并将结果反馈给高层次。</p><h1 id="研究方法和算法实现："><a href="#研究方法和算法实现：" class="headerlink" title="研究方法和算法实现："></a>研究方法和算法实现：</h1><div align="center"><img src="/2024/11/12/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E5%88%86%E9%85%8D/%E5%9F%BA%E4%BA%8E%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%AE%B6%E6%A1%86%E6%9E%B6%E7%9A%84%E5%B1%82%E6%AC%A1%E5%8C%96%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%8F%E5%90%8C%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/image-20241112210938443.png" alt="image-20241112210938443" style="zoom:40%;" align="center"></div><ol><li><strong>决策层次划分：</strong><ul><li>高层次（High Level, HL）：顶层智能体负责制定任务决策，将总任务分解并分配给底层智能体。</li><li>低层次（Low Level, LL）：底层智能体根据子任务进行动作决策，并将结果反馈给高层次。</li></ul></li><li><strong>状态空间和动作空间的分割：</strong><ul><li>根据层级关系对状态空间和指令空间进行分割，HL决策针对全局作战态势信息下达宏观作战指令，LL决策对执行宏观作战指令的作战编组进行动作操控。</li></ul></li><li><strong>上层任务分解</strong><ul><li>给定总任务<em>M</em>，拆分成子任务集{𝑀0,𝑀1,⋯,𝑀𝑖}</li><li>每一个子任务有任务类型、任务时间、任务状态和作战单元类型，可用one-hot独热码<ul><li>任务类型：主要分为打击任务和巡逻任务，打击任务包含对空拦截、对陆打击，巡逻任务包含空战巡逻、反地面战巡逻</li><li>任务时间：做离散化处理，用时刻𝑇1和时刻𝑇2来表示</li><li>任务状态：启动、未启动</li><li>作战单元：导弹驱逐舰和轰炸机</li></ul></li><li>在根据想定场景设计出子任务后，HL需要根据任务类型规划出任务启动时间，并进一步确定任务的启动次序。</li></ul></li><li><strong>奖惩函数设计：</strong><ul><li>高层智能体只需要聚焦于 子任务选择是否合适，因此高层智能体的奖励函数HL<del>r</del>设计为当任务合适时给予正奖励回报，反之则为负奖励回报。本文中根据全局任务是否完成判断子任务选择是否正确。</li></ul></li><li><strong>基于AC框架的层次化多智能体算法框架：</strong><ul><li>离策略修正的层次化学习(Hierarchical  reinforcement learning with off-policy correction,  HIRO)[31]算法是一种使用两层策略结构来解决复杂强化学习问题的一种单智能体算法，核心思想是高层策略提出目标，低层策略完成这一目标。</li><li>采用部分可观察马尔可夫决策过程（POMDP）对环境进行建模。</li><li>利用深度确定性策略梯度（Deep Deterministic Policy Gradient, DDPG）算法作为基础，结合多智能体深度强化学习，形成了Hierarchical Multi-Agent Actor-Critic（HMaAC）算法。</li></ul></li><li><strong>HMaAC算法设计：</strong><ul><li>初始化顶层和底层的Critic网络和Actor网络，以及经验回放缓冲池。</li><li>通过采样和最小化损失函数更新Critic网络，通过策略梯度更新Actor网络。</li><li>引入熵约束的概念，最大化策略的熵以学习设置合适的子目标。</li></ul></li><li><strong>仿真环境与仿真结果：</strong><ul><li>使用联合作战仿真推演平台作为实验验证环境，进行了2v2、4v4、6v6等不同规模的作战场景实验。</li><li>实验结果显示，HMaAC算法在多种复杂作战场景下均取得了较好的性能，展现了其在提升军事作战协同决策能力方面的潜力。</li></ul></li><li><strong>网络结构设计和训练参数：</strong><ul><li>设计了上层和下层的神经网络结构，均使用3层全连接层，使用ReLU和Tanh作为激活函数。</li><li>设定了一系列超参数，如最大episode数量、批处理参数、熵正则项系数等。</li></ul></li></ol><p><img src="/2024/11/12/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E5%88%86%E9%85%8D/%E5%9F%BA%E4%BA%8E%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%AE%B6%E6%A1%86%E6%9E%B6%E7%9A%84%E5%B1%82%E6%AC%A1%E5%8C%96%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%8F%E5%90%8C%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/image-20241113205648783.png"></p><ol><li><strong>训练结果分析：</strong><ul><li>对比了HMaAC算法和MADDPG算法在2v2仿真场景中的表现，HMaAC算法在奖励值和收敛速度上优于MADDPG算法。</li></ul></li></ol><p>文章最后指出，尽管HMaAC算法在实验中表现出色，但仍需在实地测试和实战演练中进一步验证其可行性和有效性，同时探讨在更复杂多样化作战环境中的适应性和鲁棒性。</p><p>[1]傅妍芳,雷凯麟,魏佳宁,等.基于演员-评论家框架的层次化多智能体协同决策方法[J].兵工学报,2024,45(10):3385-3396.</p>]]></content>
    
    
    <categories>
      
      <category>科研</category>
      
      <category>多智能体强化学习任务分配</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
      <tag>科研</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>github+hexo 搭建个人网站</title>
    <link href="/2024/11/12/hexo/github-hexo-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/"/>
    <url>/2024/11/12/hexo/github-hexo-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</url>
    
    <content type="html"><![CDATA[<h1 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h1><h2 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h2><p>Git版本：Git-2.47.0.2-64-bit</p><p>Node.js版本：node-v22.11.0-x64</p><ol><li>官网下载，一路下一步安装即可。</li></ol><p>安装完成，右键git bash here，输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git --version<br>node -v<br>npm -v<br></code></pre></td></tr></table></figure><p>安装成功，可查看版本。</p><ol start="2"><li>安装hexo:</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo -g<br></code></pre></td></tr></table></figure><p>hexo 安装成功，输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo -v<br></code></pre></td></tr></table></figure><h2 id="git-配置SSH-key"><a href="#git-配置SSH-key" class="headerlink" title="git 配置SSH key"></a>git 配置SSH key</h2><ol><li>生成key</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-keygen -t rsa -C <span class="hljs-string">&quot;xxx@qq.com&quot;</span><br></code></pre></td></tr></table></figure><ol start="2"><li>进入C:\用户\用户名.ssh，进入ssh文件夹，复制id_rsa.pub文件里的所有内容</li><li>打开github主页，点击个人设置，点击左侧SSH and GPG keys，点击New SSh key</li><li>标题随便起，将复制的内容粘贴到Key，点击Add SSH key</li><li>测试是否成功，在git bash中输入：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh -T git@github.com<br></code></pre></td></tr></table></figure><p><font style="color:rgb(83, 88, 97);">如果遇到选择，输入yes，看到成功即可。</font></p><ol start="6"><li>配置账号密码</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git config --global user.name <span class="hljs-string">&quot;xxx&quot;</span> <span class="hljs-comment">#你的github用户名</span><br>git config --global user.email <span class="hljs-string">&quot;xxx@163.com&quot;</span> <span class="hljs-comment">#填写你的github注册邮箱</span><br></code></pre></td></tr></table></figure><ol start="7"><li>安装部署插件</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-deployer-git -save<br></code></pre></td></tr></table></figure><h1 id="搭建个人博客"><a href="#搭建个人博客" class="headerlink" title="搭建个人博客"></a>搭建个人博客</h1><ol><li>新建文件夹，初始化个人博客，右键打开git bash，输入</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init <span class="hljs-comment">#初始化</span><br></code></pre></td></tr></table></figure><ol start="2"><li>生成静态网页并预览</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo g <span class="hljs-comment"># 生成静态页面</span><br>hexo s <span class="hljs-comment"># 预览</span><br></code></pre></td></tr></table></figure><ol start="3"><li><p>打开网址即可预览</p></li><li><p>打开github 创建与用户名同名的仓库：username.github.io，并查看主分支是main or master</p></li><li><p>打开博客目录下的_config.ymal，找到对应内容并修改</p></li></ol><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">deploy:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span><br>  <span class="hljs-attr">repository:</span> <span class="hljs-string">git@github.com:username/username.github.io.git</span><br>  <span class="hljs-attr">branch:</span> <span class="hljs-string">main</span><br></code></pre></td></tr></table></figure><ol start="6"><li>发布到github</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo d<br></code></pre></td></tr></table></figure><ol start="7"><li>新建博客</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new <span class="hljs-string">&#x27;blog-name&#x27;</span><br></code></pre></td></tr></table></figure><p>在source&#x2F;_post目录下可查看新建的md文件。</p><p>注：可在hexo s服务开启的状态下修改问价内容查看预览内容。</p><p>要设置其他主题，可自行查找，博主参考Fluid风格的blog <a href="https://hexo.fluid-dev.com/docs/">Hexo Fluid 用户手册</a>。</p>]]></content>
    
    
    <categories>
      
      <category>软件开发</category>
      
      <category>git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
      <tag>个人博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu虚拟机磁盘扩容</title>
    <link href="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/"/>
    <url>/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/</url>
    
    <content type="html"><![CDATA[<h1 id="Ubuntu虚拟机磁盘空间不够，如何扩容"><a href="#Ubuntu虚拟机磁盘空间不够，如何扩容" class="headerlink" title="Ubuntu虚拟机磁盘空间不够，如何扩容"></a>Ubuntu虚拟机磁盘空间不够，如何扩容</h1><h2 id="一、软件版本"><a href="#一、软件版本" class="headerlink" title="一、软件版本"></a>一、软件版本</h2><p>1.vmware 15.5.0</p><p>2.ubuntu 20.04</p><h2 id="二、操作步骤"><a href="#二、操作步骤" class="headerlink" title="二、操作步骤"></a>二、操作步骤</h2><p>1.打开虚拟机设置，点击硬盘<br><img src="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/15fc8dc4f12fa5028a9388249f3af272.png" alt="在这里插入图片描述"></p><p>2.点击扩展<br><img src="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/03bf87f8780d1f5f0d7b6c7130f738ca.png" alt="在这里插入图片描述"></p><p>3.此处我由50G增加到70G，点击扩展<br><img src="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/19983adb5563d49266bf294485b59606.png" alt="在这里插入图片描述"></p><p>4.开启虚拟机</p><p>5.点击磁盘<br><img src="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/c8428adcd11dfe842401ab61e80d16f2.png" alt="在这里插入图片描述"></p><p>6.看到有21G的未分配的磁盘空间</p><p><img src="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/226cfc5dee27ecee83abb2ac08898792.png" alt="在这里插入图片描述"></p><p>7.点击扩展分区</p><p><img src="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/e8a7616f83cfd499830719c53a862147.png" alt="在这里插入图片描述"></p><p>8.点击设置</p><p><img src="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/6b93eb112dfe8bce5b66ddb825349451.png" alt="在这里插入图片描述"></p><p>9.调整大小</p><p>10.把按钮拖动到最大，点击调整大小<br><img src="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/64180784c7a86f2bccc1103506efefda.png" alt="在这里插入图片描述"></p><p>11.选择文件系统进行同样的操作</p><p>12.在终端使用命令 df查看</p><p><img src="/2022/12/13/ubuntu/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/c8b5025526882a778881a1c67620328d.png" alt="在这里插入图片描述"></p><p>扩容成功！！！！！！！！！</p>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
