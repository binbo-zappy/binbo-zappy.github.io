

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Binbo">
  <meta name="keywords" content="">
  
    <meta name="description" content="确定策略梯度 (DPG) 确定策略梯度 (deterministic policy gradient, DPG) 是最常用的连续控制方法。DPG 是一种 actor-critic 方法，它有一个策略网络 (演员), 一个价值网络 (评委)。策略网络控制智能体做运动，它基于状态\(s\)做出动作\(a\)。价值网络不控制智能体，只是基于状态\(s\)给动作\(a\)打分，从而指导策略网络做出改">
<meta property="og:type" content="article">
<meta property="og:title" content="7.2 确定策略梯度 (DPG)">
<meta property="og:url" content="http://binbo-zappy.github.io/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/index.html">
<meta property="og:site_name" content="Binbo">
<meta property="og:description" content="确定策略梯度 (DPG) 确定策略梯度 (deterministic policy gradient, DPG) 是最常用的连续控制方法。DPG 是一种 actor-critic 方法，它有一个策略网络 (演员), 一个价值网络 (评委)。策略网络控制智能体做运动，它基于状态\(s\)做出动作\(a\)。价值网络不控制智能体，只是基于状态\(s\)给动作\(a\)打分，从而指导策略网络做出改">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://binbo-zappy.github.io/imgs/RL-WSS.png">
<meta property="article:published_time" content="2024-12-04T08:03:00.000Z">
<meta property="article:modified_time" content="2024-12-04T11:04:18.659Z">
<meta property="article:author" content="Binbo">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://binbo-zappy.github.io/imgs/RL-WSS.png">
  
  
  
  <title>7.2 确定策略梯度 (DPG) - Binbo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"binbo-zappy.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Liekkas</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="7.2 确定策略梯度 (DPG)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-12-04 16:03" pubdate>
          2024年12月4日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          8.1k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          68 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">7.2 确定策略梯度 (DPG)</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="确定策略梯度-dpg">确定策略梯度 (DPG)</h1>
<p>确定策略梯度 (deterministic policy gradient, DPG)
是最常用的连续控制方法。DPG 是一种 actor-critic 方法，它有一个策略网络
(演员), 一个价值网络 (评委)。策略网络控制智能体做运动，它基于状态<span class="math inline">\(s\)</span>做出动作<span class="math inline">\(a\)</span>。价值网络不控制智能体，只是基于状态<span class="math inline">\(s\)</span>给动作<span class="math inline">\(a\)</span>打分，从而指导策略网络做出改进。图 10.2
是两个神经网络的关系。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733276393148-f6ec8fbe-9457-4d14-a32c-78836eed2d46.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="策略网络和价值网络">策略网络和价值网络</h1>
<p>本节的策略网络不同于前面章节的策略网络。在之前章节里，策略网络<span class="math inline">\(\pi(a|s;\theta)\)</span>是一个概率质量函数，它输出的是概率值。本节的确定策略网络<span class="math inline">\(\mu(s;\theta)\)</span>的输出是<span class="math inline">\(d\)</span>维的向量<span class="math inline">\(a\)</span>,作为动作。两种策略网络一个是随机的，一个是确定性的：</p>
<ul>
<li>之前章节中的策略网络<span class="math inline">\(\pi(a|s;\theta)\)</span>带有随机性：给定状态<span class="math inline">\(s\)</span>, 策略网络输出的是离散动作空间<span class="math inline">\(A\)</span>上的概率分布；</li>
<li>本节的确定策略网络没有随机性：对于确定的状态<span class="math inline">\(s\)</span>, 策略网络<span class="math inline">\(\mu\)</span>输出的动作<span class="math inline">\(a\)</span>是确定的。动作<span class="math inline">\(a\)</span>直接是<span class="math inline">\(\mu\)</span>的输出，而非随机抽样得到的。</li>
</ul>
<p>确定策略网络<span class="math inline">\(\mu\)</span>的结构如图 10.3
所示。如果输入的状态<span class="math inline">\(s\)</span>是个矩阵或者张量
(例如图片、视频),那么<span class="math inline">\(\mu\)</span>就由若干卷积层、全连接层等组成。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733277234294-77dea5b6-056c-420c-b568-9bdfd8e824e5.png" srcset="/img/loading.gif" lazyload></p>
<p>确定策略可以看做是随机策略的一个特例。确定策略<span class="math inline">\(\mu(s;\theta)\)</span>的输出是<span class="math inline">\(d\)</span>维向量，它的第<span class="math inline">\(i\)</span>个元素记作<span class="math inline">\(\hat{\mu}_i=[\mu(s;\theta)]_i\)</span>。定义下面这个随机策略：</p>
<p><span class="math inline">\(\pi(a|s;\theta,\sigma)=\prod_{i=1}^d\frac{1}{\sqrt{6.28}\sigma_i}\cdot\exp\left(-\frac{[a_i-\hat{\mu}_i]^2}{2\sigma_i^2}\right).
\quad{(10.1)}\)</span></p>
<p>这个随机策略是均值为<span class="math inline">\(\mu(s;\theta)\)</span>、协方差矩阵为<span class="math inline">\(\text{diag}(\sigma_1,\cdots,\sigma_d)\)</span>的多元正态分布。本节的确定策略可以看做是上述随机策略在<span class="math inline">\(\sigma=[\sigma_1,\cdots,\sigma_d]\)</span>为全零向量时的特例。</p>
<p>本节的价值网络<span class="math inline">\(q(s,a;w)\)</span>是对动作价值函数<span class="math inline">\(Q_{\pi}(s,a)\)</span>的近似。价值网络的结构如图
10.4 所示。价值网络的输入是状态<span class="math inline">\(s\)</span>和动作<span class="math inline">\(a\)</span>,输出的价值<span class="math inline">\(\hat{q}=q(s,a;w)\)</span>是个实数，可以反映动作的好坏；动作<span class="math inline">\(a\)</span>越好，则价值<span class="math inline">\(\hat{q}\)</span>就越大。所以价值网络可以评价策略网络的表现。在训练的过程中，价值网络帮助训练策略网络；在训练结束之后，价值网络就被丢弃，由策略网络控制智能体。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733277392319-e425876e-ca82-4235-8d9e-8ade3dde110b.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="算法推导">算法推导</h1>
<h2 id="用行为策略收集经验">用行为策略收集经验：</h2>
<p>本节的确定策略网络属于异策略(off-policy) 方法，即行为策略 (behavior
policy) 可以不同于目标策略 (target policy)。目标策略即确定策略网络<span class="math inline">\(\mu(s;\theta_{now})\)</span>,其中<span class="math inline">\(\theta_{now}\)</span>是策略网络最新的参数。行为策略可以是任意的，比如</p>
<p><span class="math inline">\(a=\mu(s;\theta_{old})+\epsilon\)</span>.</p>
<p>公式的意思是行为策略可以用过时的策略网络参数，而且可以往动作中加入噪声<span class="math inline">\(\epsilon \in
\mathbb{R}^d\)</span>。异策略的好处在于可以把收集经验与训练神经网络分割开；把收集到的经验存入经验回放数组(replay
buffer),在做训练的时候重复利用收集到的经验。见图 10.5。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733277422761-a61031d2-0c98-4cba-903e-d7dcefa69533.png" srcset="/img/loading.gif" lazyload></p>
<p>用行为策略控制智能体与环境交互，把智能体的轨迹(trajectory)
整理成<span class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>这样的四元组，存入经验回放数组。在训练的时候，随机从数组中抽取一个四元组，记作<span class="math inline">\((s_j,a_j,r_j,s_{j+1})\)</span>。在训练策略网络<span class="math inline">\(\mu(s;\theta)\)</span>的时候，只用到状态<span class="math inline">\(s_j\)</span>。在训练价值网络<span class="math inline">\(q(s,a;w)\)</span>的时候，要用到四元组中全部四个元素：<span class="math inline">\(s_j,a_j,r_j,s_{j+1}\)</span>。</p>
<h2 id="训练策略网络">训练策略网络：</h2>
<p>首先通俗解释训练策略网络的原理。如图 10.6 所示，给定状态<span class="math inline">\(s\)</span>,策略网络输出一个动作<span class="math inline">\(a=\mu(s;\theta)\)</span>,然后价值网络会给<span class="math inline">\(a\)</span>打一个分数：<span class="math inline">\(\hat{q}=q(s,a;w)\)</span>。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733277473657-212a4de1-9f2c-44a7-9d15-d3b4a3506952.png" srcset="/img/loading.gif" lazyload></p>
<p>参数<span class="math inline">\(\theta\)</span>影响<span class="math inline">\(a\)</span>, 从而影响<span class="math inline">\(\hat{q}\)</span>。分数<span class="math inline">\(\hat{q}\)</span>可以反映出<span class="math inline">\(\theta\)</span>的好坏程度。训练策略网络的目标就是改进参数<span class="math inline">\(\theta\)</span>,使<span class="math inline">\(\hat{q}\)</span>变得更大。把策略网络看做演员，价值网络看做评委。训练演员(策略网络)的目的就是让他迎合评委
(价值网络) 的喜好，改变自己的表演技巧 (即参数<span class="math inline">\(\theta\)</span>), 使得评委打分<span class="math inline">\(\hat{q}\)</span>的均值更高。</p>
<p>根据以上解释，我们来推导目标函数。如果当前状态是<span class="math inline">\(s\)</span>, 那么价值网络的打分就是：</p>
<p><span class="math inline">\(q(s,\mu(s;\theta);w)\)</span>。</p>
<p>我们希望打分的期望尽量高，所以把目标函数定义为打分的期望：</p>
<p><span class="math inline">\(J(\theta)=\mathbb{E}_S[q(S,\mu(S;\theta);w)]\)</span>。</p>
<p>关于状态<span class="math inline">\(S\)</span>求期望消除掉了<span class="math inline">\(S\)</span>的影响；不管面对什么样的状态<span class="math inline">\(S\)</span>,
策略网络(演员)都应该做出很好的动作，使得平均分<span class="math inline">\(J(\theta)\)</span>尽量高。策略网络的学习可以建模成这样一个最大化问题：</p>
<p><span class="math inline">\(\max_{\theta}J(\theta)\)</span>。</p>
<p>注意，这里我们只训练策略网络，所以最大化问题中的优化变量是策略网络的参数<span class="math inline">\(\theta\)</span>,而价值网络的参数<span class="math inline">\(w\)</span>被固定住。</p>
<p>可以用梯度上升来增大<span class="math inline">\(J(\theta)\)</span>。每次用随机变量<span class="math inline">\(S\)</span>的一个观测值(记作<span class="math inline">\(s_j\)</span>) 来计算梯度：<span class="math inline">\(g_j \triangleq
\nabla_{\theta}q(s_j,\mu(s_j;\theta);w)\)</span>。</p>
<p>它是<span class="math inline">\(\nabla_{\theta}
J(\theta)\)</span>的无偏估计。<span class="math inline">\(g_j\)</span>叫做确定策略梯度 (deterministic policy
gradient), 缩写DPG。</p>
<p>可以用链式法则求出梯度<span class="math inline">\(g_j\)</span>。复习一下链式法则。如果有这样的函数关系<span class="math inline">\(\theta \to a \to q\)</span>,那么<span class="math inline">\(q\)</span>关于<span class="math inline">\(\theta\)</span>的导数可以写成</p>
<p><span class="math inline">\(\frac{\partial q}{\partial \theta} =
\frac{\partial a}{\partial \theta} \cdot \frac{\partial q}{\partial
a}\)</span>。</p>
<p>价值网络的输出与<span class="math inline">\(\theta\)</span>的函数关系如图 10.6
所示。应用链式法则，我们得到下面的定理。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733278437394-8e56538f-e36f-46ab-ad4b-dd94dcb05a51.png" srcset="/img/loading.gif" lazyload></p>
<p>定理 10.1. 确定策略梯度</p>
<p><span class="math inline">\(\nabla_{\theta}q(s_j,\mu(s_j;\theta);w)=\nabla_{\theta}\mu(s_j;\theta)
\cdot \nabla_a q(s_j,\hat{a}_j;w), \quad \text{其中} \quad
\hat{a}_j=\mu(s_j;\theta).\)</span></p>
<p>由此我们得到更新<span class="math inline">\(\theta\)</span>的算法。每次从经验回放数组里随机抽取一个状态，记作<span class="math inline">\(s_j\)</span>。计算<span class="math inline">\(\hat{a}_j=\mu(s_j;\theta)\)</span>。用梯度上升更新一次<span class="math inline">\(\theta\)</span>:</p>
<p><span class="math inline">\(\theta \leftarrow \theta + \beta \cdot
\nabla_{\theta}\mu(s_j;\theta) \cdot \nabla_a
q(s_j,\hat{a}_j;w).\)</span></p>
<p>此处的<span class="math inline">\(\beta\)</span>是学习率，需要手动调。这样做梯度上升，可以逐渐让目标函数<span class="math inline">\(J(\theta)\)</span>增大，也就是让评委给演员的平均打分更高。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733279212098-876f0c1c-0a9e-4abe-b074-c89d4c1be535.png" srcset="/img/loading.gif" lazyload></p>
<p>解决办法：</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733279236156-29520efa-80d5-4f6b-9fed-ebee63fc47d5.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733279312061-79ba30b5-1cc4-4573-9e0e-df095537af29.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733279390737-2cd4e7b6-a4ed-4ca9-80f5-d42b9aea27f4.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="训练价值网络">训练价值网络：</h2>
<p>首先通俗解释训练价值网络的原理。训练价值网络的目标是让价值网络<span class="math inline">\(q(s,a;w)\)</span>的预测越来越接近真实价值函数<span class="math inline">\(Q_{\pi}(s,a)\)</span>。如果把价值网络看做评委，那么训练评委的目标就是让他的打分越来越准确。每一轮训练都要用到一个实际观测的奖励<span class="math inline">\(r\)</span>,可以把<span class="math inline">\(r\)</span>看做“真理”,用它来校准评委的打分。</p>
<p>训练价值网络要用 TD 算法。这里的 TD 算法与之前学过的标准 actor-critic
类似，都是让价值网络去拟合 TD
目标。每次从经验回放数组中取出一个四元组<span class="math inline">\((s_j,a_j,r_j,s_{j+1})\)</span>,
用它更新一次参数<span class="math inline">\(w\)</span>。首先让价值网络做预测：</p>
<p><span class="math inline">\(\hat{q}_j=q(s_j,a_j;w) \quad \text{和}
\quad \hat{q}_{j+1}=q(s_{j+1},\mu(s_{j+1};\theta);w)\)</span></p>
<p>计算 TD 目标<span class="math inline">\(\hat{y}_j=r_j+\gamma \cdot
\hat{g}_{j+1}\)</span>.定义损失函数</p>
<p><span class="math inline">\(L(w)=\frac{1}{2}\left[q(s_j,a_j;w)-\hat{y}_j\right]^2,\)</span></p>
<p>计算梯度</p>
<p><span class="math inline">\(\nabla_{w} L(w) =
\underbrace{\left(\hat{q}_{j} - \hat{y}_{j}\right)}_{\text{TD误差}
\delta_{j}} \cdot \nabla_{w} q\left(s_{j}, a_{j}; w\right),\)</span></p>
<p>做一轮梯度下降更新参数<span class="math inline">\(w\)</span>:</p>
<p><span class="math inline">\(w \leftarrow w - \alpha \cdot \nabla_{w}
L(w)\)</span></p>
<p>这样可以让损失函数<span class="math inline">\(L(w)\)</span>减小，也就是让价值网络的预测<span class="math inline">\(\hat{q}_j=q(s,a;w)\)</span>更接近 TD 目标<span class="math inline">\(\hat{y}_j\)</span>。公式中的<span class="math inline">\(\alpha\)</span>是学习率，需要手动调。</p>
<p><strong>训练流程</strong>：<br>
做训练的时候，可以同时对价值网络和策略网络做训练。每次从经验回放数组中抽取一个四元组，记作<span class="math inline">\((s_j,a_j,r_j,s_{j+1})\)</span>。把神经网络当前参数记作<span class="math inline">\(w_{now}\)</span>和<span class="math inline">\(\theta_{now}\)</span>。执行以下步骤更新策略网络和价值网络：</p>
<ol type="1">
<li>让策略网络做预测：<span class="math inline">\(\hat{a}_j=\mu(s_j;\theta_{now})\)</span>和<span class="math inline">\(\hat{a}_{j+1}=\mu(s_{j+1};\theta_{now})\)</span>.</li>
<li>让价值网络做预测：<span class="math inline">\(\hat{q}_j=q(s_j,a_j;w_{now})\)</span>和<span class="math inline">\(\hat{q}_{j+1}=q(s_{j+1},\hat{a}_{j+1};w_{now})\)</span>.</li>
<li>计算 TD 目标和 TD 误差：<span class="math inline">\(\hat{y}_j=r_j+\gamma \cdot
\hat{q}_{j+1}\)</span>和<span class="math inline">\(\delta_j=\hat{q}_j-\hat{y}_j\)</span>.</li>
<li>更新价值网络：<span class="math inline">\(w_{new}=w_{now}-\alpha
\cdot \delta_j \cdot \nabla_w q(s_j,a_j;w_{now})\)</span>.</li>
<li>更新策略网络：<span class="math inline">\(\theta_{new}=\theta_{now}+\beta \cdot
\nabla_{\theta}\mu(s_j;\theta_{now}) \cdot \nabla_a
q(s_j,\hat{a}j;w_{now})\)</span>.</li>
</ol>
<p>在实践中，上述算法的表现并不好；读者应当采用后面介绍的技巧训练策略网络和价值网络。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733277619102-cbf150fb-2913-49e7-afa0-e8d5499baa46.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="深入分析-dpg">深入分析 DPG</h1>
<p>上一节介绍的 DPG 是一种“四不像”的方法。DPG
乍看起来很像前面介绍的策略学习方法，因为 DPG 的目的是学习一个策略<span class="math inline">\(\mu\)</span>,而价值网络<span class="math inline">\(q\)</span>只起辅助作用。然而 DPG 又很像之前介绍的
DQN, 两者都是异策略 (Off-policy), 而且两者存在高估问题。鉴于 DPG
的重要性，我们更深入分析 DPG。</p>
<h2 id="从策略学习的角度看待-dpg">从策略学习的角度看待 DPG</h2>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733280642243-5fa19640-5b83-438e-aaee-e64bff97fa84.png" srcset="/img/loading.gif" lazyload></p>
<p>答案是动作价值函数<span class="math inline">\(Q_{\pi}(s,a)\)</span>。上一节 DPG
的训练流程中，更新价值网络用到 TD 目标：<span class="math inline">\(\hat{y}_j=r_j+\gamma \cdot
q(s_{j+1},\mu(s_{j+1};\theta_{now});w_{now})\)</span>。</p>
<p>很显然，当前的策略<span class="math inline">\(\mu(s;\theta_{now})\)</span>会直接影响价值网络<span class="math inline">\(q\)</span>。策略不同，得到的价值网络<span class="math inline">\(q\)</span>就不同。</p>
<p>虽然价值网络<span class="math inline">\(q(s,a;w)\)</span>通常是对动作价值函数<span class="math inline">\(Q_{\pi}(s,a)\)</span>的近似，但是我们最终的目标是让<span class="math inline">\(q(s,a;w)\)</span>趋近于最优动作价值函数<span class="math inline">\(Q^{\star}(s,a)\)</span>。回忆一下，如果<span class="math inline">\(\pi\)</span>是最优策略<span class="math inline">\(\pi^{\star}\)</span>, 那么<span class="math inline">\(Q_{\pi}(s,a)\)</span>就等于<span class="math inline">\(Q^{\star}(s,a)\)</span>。训练 DPG 的目的是让<span class="math inline">\(\mu(s;\theta)\)</span>趋近于最优策略<span class="math inline">\(\pi^{\star}\)</span>那么理想情况下，<span class="math inline">\(q(s,a;w)\)</span>最终趋近于<span class="math inline">\(Q^{\star}(s,a)\)</span>。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733280692694-384edfe4-ceed-417b-9fbf-a933c5a2e5a4.png" srcset="/img/loading.gif" lazyload></p>
<p>答案是目标策略<span class="math inline">\(\mu(s;\theta_{now})\)</span>,因为目标策略对价值网络的影响很大。在理想情况下，行为策略对价值网络没有影响。我们用
TD 算法训练价值网络，TD 算法的目的在于鼓励价值网络的预测趋近于 TD
目标。理想情况下，</p>
<p><span class="math inline">\(q(s_j,a_j;w)=r_j+\gamma \cdot
Q(s_{j+1},\mu(s_{j+1};\theta_{now});w_{now}), \quad \forall
(s_j,a_j,r_j,s_{j+1}).\)</span></p>
<p>在收集经验的过程中，行为策略决定了如何基于<span class="math inline">\(s_j\)</span>生成<span class="math inline">\(a_j\)</span>,然而这不重要。上面的公式只希望等式左边去拟合等式右边，而不在乎<span class="math inline">\(a_j\)</span>是如何生成的。</p>
<h2 id="从价值学习的角度看待-dpg">从价值学习的角度看待 DPG</h2>
<p>假如我们知道最优动作价值函数<span class="math inline">\(Q^{\star}(s,a;w)\)</span>,我们可以这样做决策：给定当前状态<span class="math inline">\(s_t\)</span>,选择最大化 Q 值的动作</p>
<p><span class="math inline">\(a_t=\arg\max_{a \in A}
Q^{\star}(s_t,a)\)</span>.</p>
<p>DQN 记作<span class="math inline">\(Q(s,a;w)\)</span>,它是<span class="math inline">\(Q^{\star}(s,a;w)\)</span>的函数近似。训练 DQN
的目的是让<span class="math inline">\(Q(s,a;w)\)</span>趋近<span class="math inline">\(Q^{\star}(s,a;w)\)</span>,<span class="math inline">\(\forall s \in S, a \in A\)</span>。在训练好 DQN
之后，可以这样做决策：</p>
<p><span class="math inline">\(a_t=\arg\max_{a \in A}
Q(s_t,a;w).\)</span></p>
<p>如果动作空间<span class="math inline">\(A\)</span>是离散集合，那么上述最大化很容易实现。可是如果<span class="math inline">\(A\)</span>是连续集合，则很难对<span class="math inline">\(Q\)</span>求最大化。</p>
<p>可以把 DPG 看做对最优动作价值函数<span class="math inline">\(Q^{\star}(s,a)\)</span>的另一种近似方式，用于连续控制问题。我们希望学到策略网络<span class="math inline">\(\mu(s;\theta)\)</span>和价值网络<span class="math inline">\(q(s,a;w)\)</span>, 使得</p>
<p><span class="math inline">\(q(s,\mu(s;\theta);w) \approx \max_{a \in
A} Q^{\star}(s,a), \quad \forall s \in S.\)</span></p>
<p>我们可以把<span class="math inline">\(\mu\)</span>和<span class="math inline">\(q\)</span>看做是<span class="math inline">\(Q^{\star}\)</span>的近似分解，而这种分解的目的在于方便做决策：</p>
<p><span class="math inline">\(a_t = \mu(s_t;\theta) \approx \arg\max_{a
\in A} Q^{\star}(s_t,a).\)</span></p>
<h1 id="dpg-的高估问题">DPG 的高估问题</h1>
<p>在之前的笔记 深度强化学习（王树森）笔记08-CSDN博客，我们讨过 DQN
的高估问题：如果用 Q 学习算法训练 DQN, 则 DQN
会高估真实最优价值函数<span class="math inline">\(Q^{\star}\)</span>。把
DQN 记作<span class="math inline">\(Q(s,a;w)\)</span>。如果用 Q
学习算法训练 DQN, 那么 TD 目标是</p>
<p><span class="math inline">\(\hat{y}_j=r_j+\gamma \cdot \max_{a \in A}
Q(s_{j+1},a;w).\)</span></p>
<p>之前得出结论：如果<span class="math inline">\(Q(s,a;w)\)</span>是最优动作价值函数<span class="math inline">\(Q^{\star}(s,a)\)</span>的无偏估计，那么<span class="math inline">\(\hat{y}_j\)</span>是对<span class="math inline">\(Q^{\star}(s_j,a_j)\)</span>的高估。用<span class="math inline">\(\hat{y}_j\)</span>作为目标去更新 DQN, 会导致<span class="math inline">\(Q(s_j,a_j;w)\)</span>高估<span class="math inline">\(Q^{\star}(s_j,a_j)\)</span>。另一个结论是自举会导致高估的传播，造成高估越来越严重。</p>
<p>DPG 也存在高估问题，用上一节的算法训练出的价值网络<span class="math inline">\(q(s,a;w)\)</span>会高估真实动作价值<span class="math inline">\(Q_{\pi}(s,a)\)</span>。造成 DPG 高估的原因与 DQN
类似：第一，TD
目标是对真实动作价值的高估；第二，自举导致高估的传播。下面具体分析两个原因；如果读者不感兴趣，只需要记住上述结论即可，可以跳过下面的内容。</p>
<p><strong>最大化造成高估</strong>：<br>
训练策略网络的时候，我们希望策略网络计算出的动作<span class="math inline">\(\hat{a}=a=\mu(s;\theta)\)</span>能得到价值网络尽量高的评价，也就是让<span class="math inline">\(q(s,\hat{a};w)\)</span>尽量大。我们通过求解下面的优化模型来学习策略网络：</p>
<p><span class="math inline">\(\theta^{\star}=\operatorname{argmax}_{\theta}\mathbb{E}_S[q(S,\hat{A};w)],
\quad \text{s.t.} \quad \hat{A}=\mu(S;\theta).\)</span></p>
<p>这个公式的意思是<span class="math inline">\(\mu(s;\theta^{\star})\)</span>是最优的确定策略网络。
上面的公式与下面的公式意义相同(虽然不严格等价)：</p>
<p><span class="math inline">\(\mu(s;\theta^{\star})=\arg\max_{a \in A}
q(s,a;w), \quad \forall s \in S.\)</span></p>
<p>这个公式的意思也是<span class="math inline">\(\mu(s;\theta^{\star})\)</span>是最优的确定策略网络。训练价值网络<span class="math inline">\(q\)</span>时用的 TD 目标是</p>
<p><span class="math inline">\(\begin{align*}
\widehat{y}_{j} &amp;= r_{j} + \gamma \cdot q\left(s_{j+1},
\mu\left(s_{j+1}; \theta\right); w\right) \\
&amp;\approx r_{j} + \gamma \cdot \max_{a_{j+1}} q\left(s_{j+1},
a_{j+1}; w\right).
\end{align*}\)</span></p>
<p>根据前面的分析，上面公式中的<span class="math inline">\(\max\)</span>会导致<span class="math inline">\(\hat{y}_j\)</span>高估真实动作价值<span class="math inline">\(Q_{\pi}(s_j,a_j)\)</span>。在训练<span class="math inline">\(q\)</span>时，我们把<span class="math inline">\(\hat{y}_j\)</span>作为目标，鼓励价值网络<span class="math inline">\(q(s_j,a_j;w)\)</span>接近<span class="math inline">\(\hat{y}_j\)</span>, 这会导致<span class="math inline">\(q(s_j,a_j;w)\)</span>高估真实动作价值。</p>
<p><strong>自举造成偏差传播</strong>：<br>
前面的笔记中讨论过自举(bootstrapping)造成偏差的传播。</p>
<p>TD 目标</p>
<p><span class="math inline">\(\hat{y}_j = r_j + \gamma \cdot
q(s_{j+1},\mu(s_{j+1};\theta);w)\)</span></p>
<p>是用价值网络算出来的，而它又被用于更新价值网络<span class="math inline">\(q\)</span>本身，这属于自举。假如价值网络<span class="math inline">\(q(s_{j+1},a_{j+1};\theta)\)</span>高估了真实动作价值<span class="math inline">\(Q_{\pi}(s_{j+1},a_{j+1})\)</span>,那么 TD
目标<span class="math inline">\(\hat{y}_j\)</span>则是对<span class="math inline">\(Q_{\pi}(s_j,a_j)\)</span>的高估，这会导致<span class="math inline">\(q(s_j,a_j;w)\)</span>高估<span class="math inline">\(Q_{\pi}(s_j,a_j)\)</span>。自举让高估从<span class="math inline">\((s_{j+1},a_{j+1})\)</span>传播到<span class="math inline">\((s_j,a_j)\)</span>。</p>
<h1 id="双延时确定策略梯度-td3">双延时确定策略梯度 (TD3)</h1>
<p>由于存在高估等问题，DPG 实际运行的效果并不好。本节介绍的 Twin Delayed
Deep Deterministic Policy Gradient (TD3)
可以大幅提升算法的表现，把策略网络和价值网络训练得更好。注意，本节只是改进训练用的算法，并不改变神经网络的结构。</p>
<h2 id="高估问题的解决方案">高估问题的解决方案</h2>
<p><strong>解决方案——目标网络</strong>：<br>
为了解决自举和最大化造成的高估，我们需要使用目标网络 (Target Networks)
计算 TD 目标<span class="math inline">\(\hat{y}_j\)</span>。训练中需要两个目标网络：</p>
<p><span class="math inline">\(q(s,a;w^-)\)</span>和<span class="math inline">\(\mu(s;\theta^-)\)</span>.</p>
<p>它们与价值网络、策略网络的结构完全相同，但是参数不同。TD
目标是用目标网络算的：</p>
<p><span class="math inline">\(\hat{y}_j = r_j + \gamma \cdot
q(s_{j+1},\hat{a}_{j+1};w^-), \quad \text{其中} \quad
\hat{a}_{j+1}=\mu(s_{j+1};\theta^-).\)</span></p>
<p>把<span class="math inline">\(\hat{y}_j\)</span>作为目标，更新<span class="math inline">\(w\)</span>,鼓励<span class="math inline">\(q(s_j,a_j;w)\)</span>接近<span class="math inline">\(\hat{y}_j\)</span>。四个神经网络之间的关系如图
10.7所示。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733280970215-074c9b2c-1dcd-43bc-a735-92cf1c805a34.png" srcset="/img/loading.gif" lazyload></p>
<p>这种方法可以在一定程度上缓解高估，但是实验表明高估仍然很严重。</p>
<p><strong>更好的解决方案——截断双 Q 学习 (clipped double
Q-learning)</strong>:<br>
这种方法使用两个价值网络和一个策略网络：</p>
<p><span class="math inline">\(q(s,a;w_1)\)</span>,<span class="math inline">\(q(s,a;w_2)\)</span>,<span class="math inline">\(\mu(s;\theta)\)</span>.</p>
<p>三个神经网络各对应一个目标网络：</p>
<p><span class="math inline">\(q(s,a;w_1^-)\)</span>,<span class="math inline">\(q(s,a;w_2^-)\)</span>,<span class="math inline">\(\mu(s;\theta^-)\)</span>.</p>
<p>用目标策略网络计算动作：</p>
<p><span class="math inline">\(\hat{a}_{j+1}^-=\mu(s_{j+1};\theta^-),\)</span></p>
<p>然后用两个目标价值网络计算：</p>
<p><span class="math inline">\(\hat{y}_{j,1} = r_j + \gamma \cdot
q(s_{j+1},\hat{a}_{j+1}^-;w_1^-),\)</span></p>
<p><span class="math inline">\(\hat{y}_{j,2} = r_j + \gamma \cdot
q(s_{j+1},\hat{a}_{j+1}^-;w_2^-).\)</span></p>
<p>取两者较小者为TD 目标：</p>
<p><span class="math inline">\(\hat{y}_j = \min\{\hat{y}_{j,1},
\hat{y}_{j,2}\}.\)</span></p>
<p><font style="color:rgb(77, 77, 77);">截断双 Q
学习中的六个神经网络的关系如图 10.8 所示。</font></p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733281046414-16ca9237-4b69-43a3-a03c-a06bfa4062ad.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="其他改进方法">其他改进方法</h2>
<p>可以在截断双 Q
学习算法的基础上做两处小的改进、进一步提升算法的表现。两种改进分别是往动作中加噪声、减小更新策略网络和目标网络的频率。</p>
<p><strong>往动作中加噪声</strong>：<br>
上一小节中截断双 Q 学习用目标策略网络计算动作：</p>
<p><span class="math inline">\(\hat{a}_{j+1}^-=\mu(s_{j+1};\theta^-).\)</span></p>
<p>把这一步改成：</p>
<p><span class="math inline">\(\hat{a}_{j+1}^-=\mu(s_{j+1};\theta^-)+\xi.\)</span></p>
<p>公式中的<span class="math inline">\(\xi\)</span>是个随机向量，表示噪声，它的每一个元素独立随机从截断正态分布<span class="math inline">\(\mathcal{CN}(0,\sigma^2,-c,c)\)</span>中抽取。把截断正态分布记作<span class="math inline">\(\mathcal{CN}(0,\sigma^2,-c,c)\)</span>,意思是均值为零，标准差为<span class="math inline">\(\sigma\)</span>的正态分布，但是变量落在区间<span class="math inline">\([-c,c]\)</span>之外的概率为零。正态分布与截断正态分布的对比如图
10.9 所示。使用截断正态分布，而非正态分布，是为了防止噪声<span class="math inline">\(\xi\)</span>过大。使用截断，保证噪声大小不会超过
-c 和 c。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733281171745-ab054b7e-eddc-4142-9319-678bd6ea4d3c.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>减小更新策略网络和目标网络的频率</strong>：</p>
<p>Actor-critic 用价值网络来指导策略网络的更新。如果价值网络 q
本身不可靠，那么用价值网络 q
给动作打的分数是不准确的，无助于改进策略网络<span class="math inline">\(\mu\)</span>。在价值网络 q
还很差的时候就急于更新<span class="math inline">\(\mu\)</span>,
非但不能改进<span class="math inline">\(\mu\)</span>,反而会由于<span class="math inline">\(\mu\)</span>的变化导致 q 的训练不稳定。<br>
实验表明，应当让策略网络<span class="math inline">\(\mu\)</span>以及三个目标网络的更新慢于价值网络<span class="math inline">\(q\)</span>。传统的actor-critic
的每一轮训练都对策略网络、价值网络、以及目标网络做一次更新。更好的方法是每一轮更新一次价值网络，但是每隔<span class="math inline">\(k\)</span>轮更新一次策略网络和三个目标网络。<span class="math inline">\(k\)</span>是超参数，需要调。</p>
<h3 id="训练流程">训练流程</h3>
<p>本节介绍了三种技巧，改进 DPG 的训练。第一，用截断双 Q
学习，缓解价值网络的高估。第二，往目标策略网络中加噪声，起到平滑作用。第三，降低策略网络和三个目标网络的更新频率。使用这三种技巧的算法被称作双延时确定策略梯度
(twin delayed deep deterministic policy gradient), 缩写是 TD3。</p>
<p>TD3 与 DPG 都属于异策略 (off-policy),
可以用任意的行为策略收集经验，事后做经验回放训练策略网络和价值网络。收集经验的方式与原始的训练算法相同，用<span class="math inline">\(\mu(s_t;\theta)+\epsilon\)</span>与环境交互，把观测到的四元组<span class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>存入经验回放数组。</p>
<p>初始的时候，策略网络和价值网络的参数都是随机的。这样初始化目标网络的参数：</p>
<p><span class="math inline">\(w_1^- \leftarrow w_1, \quad w_2^-
\leftarrow w_2, \quad \theta^- \leftarrow \theta.\)</span></p>
<p>训练策略网络和价值网络的时候，每次从数组中随机抽取一个四元组，记作<span class="math inline">\((s_j,a_j,r_j,s_{j+1})\)</span>。用下标 now
表示神经网络当前的参数，用下标 new
表示更新后的参数。然后执行下面的步骤，更新价值网络、策略网络、目标网络。</p>
<ol type="1">
<li>让目标策略网络做预测：</li>
</ol>
<p>$<em>{j+1}<sup>-=(s_{j+1};</sup>-</em>{now})+. $</p>
<p>其中向量<span class="math inline">\(\xi\)</span>的每个元素都独立从截断正态分布<span class="math inline">\(\mathcal{CN}(0,\sigma^2,-c,c)\)</span>中抽取.</p>
<ol start="2" type="1">
<li>让两个目标价值网络做预测：</li>
</ol>
<p><span class="math inline">\(\hat{q}_{1,j+1}^-=q(s_{j+1},\hat{a}_{j+1}^-;w_{1,now}^-)
\quad \text{和} \quad
\hat{q}_{2,j+1}^-=q(s_{j+1},\hat{a}_{j+1}^-;w_{2,now}^-).\)</span></p>
<ol start="3" type="1">
<li>计算 TD 目标：</li>
</ol>
<p><span class="math inline">\(\hat{y}_j = r_j + \gamma \cdot
\min\{\hat{q}_{1,j+1}^-, \hat{q}_{2,j+1}^-\}.\)</span></p>
<ol start="4" type="1">
<li>让两个价值网络做预测：</li>
</ol>
<p><span class="math inline">\(\hat{q}_{1,j}=q(s_j,a_j;w_{1,now}) \quad
\text{和} \quad \hat{q}_{2,j}=q(s_j,a_j;w_{2,now}).\)</span></p>
<ol start="5" type="1">
<li>计算 TD 误差：</li>
</ol>
<p><span class="math inline">\(\delta_{1,j}=\hat{q}_{1,j}-\hat{y}_j
\quad \text{和} \quad \delta_{2,j}=\hat{q}_{2,j}-\hat{y}_j.\)</span></p>
<ol start="6" type="1">
<li>更新价值网络：</li>
</ol>
<p><span class="math inline">\(w_{1,new} \leftarrow w_{1,now} - \alpha
\cdot \delta_{1,j} \cdot \nabla_w q(s_j,a_j;w_{1,now}),\)</span></p>
<p><span class="math inline">\(w_{2,new} \leftarrow w_{2,now} - \alpha
\cdot \delta_{2,j} \cdot \nabla_w q(s_j,a_j;w_{2,now}).\)</span></p>
<ol start="7" type="1">
<li>每隔<span class="math inline">\(k\)</span>轮更新一次策略网络和三个目标网络：</li>
</ol>
<ul>
<li>让策略网络做预测：</li>
</ul>
<p><span class="math inline">\(\hat{a}_j=\mu(s_j;\theta_{now}). \quad
\text{然后更新策略网络：}\)</span></p>
<p><span class="math inline">\(\theta_{new} \leftarrow \theta_{now} +
\beta \cdot \nabla_{\theta}\mu(s_j;\theta_{now}) \cdot \nabla_a
q(s_j,\hat{a}_j;w_{1,now}).\)</span></p>
<ul>
<li>更新目标网络的参数：</li>
</ul>
<p><span class="math inline">\(\theta_{new}^- \leftarrow \tau \cdot
\theta_{new} + (1-\tau) \cdot \theta_{now}^-,\)</span></p>
<p><span class="math inline">\(w_{1,new}^- \leftarrow \tau \cdot
w_{1,new} + (1-\tau) \cdot w_{1,now}^-,\)</span></p>
<p><span class="math inline">\(w_{2,new}^- \leftarrow \tau \cdot
w_{2,new} + (1-\tau) \cdot w_{2,now}^-.\)</span></p>
<h1 id="随机高斯策略">随机高斯策略</h1>
<p>上一节用确定策略网络解决连续控制问题。本节用不同的方法做连续控制，本节的策略网络是随机的，它是随机正态分布
(也叫高斯分布)。</p>
<h2 id="基本思路">基本思路</h2>
<p>我们先研究最简单的情形：自由度等于 1, 也就是说动作<span class="math inline">\(a\)</span>是实数，动作空间<span class="math inline">\(A \subset
\mathbb{R}\)</span>。把动作的均值记作<span class="math inline">\(\mu(s)\)</span>,标准差记作<span class="math inline">\(\sigma(s)\)</span>, 它们都是状态<span class="math inline">\(s\)</span>的函数。用正态分布的概率密度函数作为策略函数：<span class="math inline">\(\pi(a|s)=\frac{1}{\sqrt{6.28}\cdot\sigma(s)}\cdot\exp\left(-\frac{(a-\mu(s))^2}{2\cdot\sigma^2(s)}\right).
\quad (10.2)\)</span></p>
<p>假如我们知道函数<span class="math inline">\(\mu(s)\)</span>和<span class="math inline">\(\sigma(s)\)</span>的解析表达式，可以这样做控制：</p>
<ol type="1">
<li>观测到当前状态<span class="math inline">\(s\)</span>, 预测均值<span class="math inline">\(\widehat{\mu} = \mu(s)\)</span>和标准差<span class="math inline">\(\widehat{\sigma} = \sigma(s).\)</span>。</li>
<li>从正态分布中做随机抽样<span class="math inline">\(a \sim
N(\mu^,\sigma^2)\)</span>; 智能体执行动作<span class="math inline">\(a\)</span>。</li>
</ol>
<p>然而我们并不知道<span class="math inline">\(\mu(s)\)</span>和<span class="math inline">\(\sigma(s)\)</span>是怎么样的函数。一个很自然的想法是用神经网络来近似这两个函数。把神经网络记作<span class="math inline">\(\mu(s;\theta)\)</span>和<span class="math inline">\(\rho(s;\theta)\)</span>,其中<span class="math inline">\(\theta\)</span>表示神经网络中的可训练参数。但实践中最好不要直接近似标准差<span class="math inline">\(\sigma\)</span>,而是近似方差对数<span class="math inline">\(\ln \sigma^2\)</span>。定义两个神经网络：</p>
<p><span class="math inline">\(\mu(s;\theta) \quad \text{和} \quad
\rho(s;\theta),\)</span></p>
<p>分别用于预测均值和方差对数。可以按照图10.10来搭建神经网络。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733281722169-2371d191-945c-4db2-b338-ccb673693049.png" srcset="/img/loading.gif" lazyload></p>
<p>神经网络的输入是状态<span class="math inline">\(s\)</span>,通常是向量、矩阵或者张量。神经网络有两个输出头，分别记作<span class="math inline">\(\mu(s;\theta)\)</span>和<span class="math inline">\(\rho(s;\theta)\)</span>可以这样用神经网络做控制：</p>
<ol type="1">
<li>观测到当前状态<span class="math inline">\(s\)</span>, 计算均值<span class="math inline">\(\widehat{\mu} = \mu(s;\theta)\)</span>,
方差对数<span class="math inline">\(\widehat{\rho} =
\rho(s;\theta)\)</span>, 以及方差$ ^2 = ()$.</li>
<li>从正态分布中做随机抽样：<span class="math inline">\(a \sim
\mathcal{N}(\widehat{\mu}, \widehat{\sigma}^2)\)</span>;
智能体执行动作<span class="math inline">\(a\)</span>.</li>
</ol>
<p>用神经网络近似均值和标准差之后，公式 (10.2) 中的策略函数<span class="math inline">\(\pi(a|s)\)</span>变成了下面的策略网络：</p>
<p><span class="math inline">\(\pi(a|s;\theta)=\frac{1}{\sqrt{6.28\cdot\exp[\rho(s;\theta)]}}\cdot\exp\left(-\frac{(a-\mu(s;\theta))^2}{2\cdot\exp[\rho(s;\theta)]}\right).\)</span></p>
<p>实际做控制的时候，我们只需要神经网络<span class="math inline">\(\mu(s;\theta)\)</span>和<span class="math inline">\(\rho(s;\theta)\)</span>,用不到真正的策略网络<span class="math inline">\(\pi(a|s;\theta)\)</span>.</p>
<h2 id="随机高斯策略网络">随机高斯策略网络</h2>
<p>上一小节假设控制问题的自由度是<span class="math inline">\(d=1\)</span>,也就是说动作<span class="math inline">\(a\)</span>是标量。实际问题中的自由度<span class="math inline">\(d\)</span>往往大于 1, 那么动作<span class="math inline">\(a\)</span>是<span class="math inline">\(d\)</span>维向量。对于这样的问题，我们修改一下神经网络结构，让两个输出<span class="math inline">\(\mu(s;\theta)\)</span>和<span class="math inline">\(\rho(s;\theta)\)</span>都<span class="math inline">\(d\)</span>维向量；见图10.11。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733282010401-80abc816-ce3a-42ed-9859-abadc0c73217.png" srcset="/img/loading.gif" lazyload></p>
<p>用标量<span class="math inline">\(a_i\)</span>表示动作向量<span class="math inline">\(a\)</span>的第<span class="math inline">\(i\)</span>个元素。用函数<span class="math inline">\(\mu_i(s;\theta)\)</span>和<span class="math inline">\(\rho_i(s;\theta)\)</span>分别表示<span class="math inline">\(\mu(s;\theta)\)</span>和<span class="math inline">\(\rho(s;\theta)\)</span>的第<span class="math inline">\(i\)</span>个元素。我们用下面这个特殊的多元正态分布的概率密度函数作为策略网络：</p>
<p><span class="math inline">\(\pi(a|s;\theta)=\prod_{i=1}^{d}\frac{1}{\sqrt{6.28\cdot\exp[\rho_i(s;\theta)]}}\cdot\exp\left(-\frac{(a_i-\mu_i(s;\theta))^2}{2\cdot\exp[\rho_i(s;\theta)]}\right).\)</span></p>
<p>做控制的时候只需要均值网络<span class="math inline">\(\mu(s;\theta)\)</span>和方差对数网络<span class="math inline">\(\rho(s;\theta)\)</span>,不需要策略网络<span class="math inline">\(\pi(a|s;\theta)\)</span>做训练的时候也不需要<span class="math inline">\(\pi(a|s;\theta)\)</span>, 而是要用辅助网络<span class="math inline">\(f(s,a;\theta)\)</span>。总而言之，策略网络<span class="math inline">\(\pi\)</span>只是帮助你理解本节的方法而已，实际算法中不会出现<span class="math inline">\(\pi\)</span>。</p>
<p>图10.11描述了辅助网络<span class="math inline">\(f(s,a;\theta)\)</span>与<span class="math inline">\(\mu\)</span>、<span class="math inline">\(\rho\)</span>、<span class="math inline">\(a\)</span>的关系。辅助网络具体是这样定义的：</p>
<p><span class="math inline">\(f(s,a;\theta)=-\frac{1}{2}\sum_{i=1}^{d}\left(\rho_i(s;\theta)+\frac{(a_i-\mu_i(s;\theta))^2}{\exp[\rho_i(s;\theta)]}\right).\)</span></p>
<p>它的可训练参数<span class="math inline">\(\theta\)</span>都是从<span class="math inline">\(\mu(s;\theta)\)</span>和<span class="math inline">\(\rho(s;\theta)\)</span>中来的。不难发现，辅助网络与策略网络有这样的关系：</p>
<p><span class="math inline">\(f(s,a;\theta)=\ln\pi(a|s;\theta)+\text{Constant}.
\quad (10.3)\)</span></p>
<h2 id="策略梯度">策略梯度</h2>
<p>回忆一下之前学过的内容。在<span class="math inline">\(t\)</span>时刻的折扣回报记作随机变量<span class="math inline">\(U_t=R_t+\gamma \cdot R_{t+1}+\gamma^2 \cdot
R_{t+2}+\cdots+\gamma^{n-t} \cdot R_n\)</span>.</p>
<p>动作价值函数<span class="math inline">\(Q_{\pi}(s_t,a_t)\)</span>是对折扣回报<span class="math inline">\(U_t\)</span>的条件期望。前面章节推导过策略梯度的蒙特卡洛近似：</p>
<p><span class="math inline">\(g=Q_{\pi}(s,a) \cdot
\nabla_{\theta}\ln\pi(a|s;\theta).\)</span></p>
<p>由公式 (10.3) 可得：</p>
<p><span class="math inline">\(g=Q_{\pi}(s,a) \cdot
\nabla_{\theta}f(s,a;\theta). \quad (10.4)\)</span></p>
<p>有了策略梯度，就可以学习参数<span class="math inline">\(\theta\)</span>。训练的过程大致如下：</p>
<ol type="1">
<li><p>搭建均值网络<span class="math inline">\(\mu(s;\theta)\)</span>、方差对数网络<span class="math inline">\(\rho(s;\theta)\)</span>、辅助网络<span class="math inline">\(f(s,a;\theta)\)</span>。</p></li>
<li><p>让智能体与环境交互，记录每一步的状态、动作、奖励，并对参数<span class="math inline">\(\theta\)</span>做更新：</p>
<p>(a). 观测到当前状态<span class="math inline">\(s\)</span>,计算均值、方差对数、方差：<span class="math inline">\(\hat{\mu}=\mu(s;\theta)\)</span>,<span class="math inline">\(\hat{\rho}=\rho(s;\theta)\)</span>,<span class="math inline">\(\hat{\sigma}^2=\exp(\hat{\rho})\)</span>.</p>
<p>此处的指数函数<span class="math inline">\(\exp(\cdot)\)</span>应用到向量的每一个元素上。</p>
<p>(b). 设<span class="math inline">\(\hat{\mu}_i\)</span>和<span class="math inline">\(\hat{\sigma}_i\)</span>分别是<span class="math inline">\(d\)</span>维向量<span class="math inline">\(\hat{\mu}\)</span>和<span class="math inline">\(\hat{\sigma}\)</span>的第<span class="math inline">\(i\)</span>个元素。从正态分布中做抽样：<span class="math inline">\(a_i \sim
N(\hat{\mu}_i,\hat{\sigma}_i^2)\)</span>,<span class="math inline">\(\forall i=1,\cdots,d\)</span>.
把得到的动作记作<span class="math inline">\(a=[a_1,\cdots,a_d]\)</span>.</p>
<p>(c). 近似计算动作价值：<span class="math inline">\(\hat{q} \approx
Q_{\pi}(s,a)\)</span>.</p>
<p>(d). 用反向传播计算出辅助网络关于参数<span class="math inline">\(\theta\)</span>的梯度：<span class="math inline">\(\nabla_{\theta}f(s,a;\theta)\)</span>.</p>
<p>(e). 用策略梯度上升更新参数：<span class="math inline">\(\theta
\leftarrow \theta + \beta \cdot \hat{q} \cdot
\nabla_{\theta}f(s,a;\theta)\)</span>. 此处的<span class="math inline">\(\beta\)</span>是学习率。</p></li>
</ol>
<h3 id="用-reinforce-学习参数">用 REINFORCE 学习参数</h3>
<p>REINFORCE 用实际观测的折扣回报<span class="math inline">\(u_t=\sum_{k=t}^n\gamma^{k-t} \cdot
r_k\)</span>代替动作价值<span class="math inline">\(Q_{\pi}(s_t,a_t)\)</span>。</p>
<p>道理是这样的。动作价值是回报的期望：</p>
<p><span class="math inline">\(Q_{\pi}(s_t,a_t)=\mathbb{E}[U_t|S_t=s_t,A_t=a_t]\)</span>.</p>
<p>随机变量<span class="math inline">\(U_t\)</span>的一个实际观测值<span class="math inline">\(u_t\)</span>是期望的蒙特卡洛近似。这样一来，公式
(10.4) 中的策略梯度就能近似成<span class="math inline">\(g \approx u_t
\cdot \nabla_{\theta}f(s,a;\theta)\)</span>.</p>
<p>在搭建好均值网络<span class="math inline">\(\mu(s;\theta)\)</span>、方差对数网络<span class="math inline">\(\rho(s;\theta)\)</span>、辅助网络<span class="math inline">\(f(s,a;\theta)\)</span>之后，我们用REINFORCE
更新参数<span class="math inline">\(\theta\)</span>。设当前参数为<span class="math inline">\(\theta_{now}\)</span>。REINFORCE
重复以下步骤，直到收敛：</p>
<ol type="1">
<li>用<span class="math inline">\(\mu(s;\theta_{now})\)</span>和<span class="math inline">\(\rho(s;\theta_{now})\)</span>控制智能体与环境交互，完成一局游戏，得到一条轨迹<span class="math inline">\(s_1,a_1,r_1,s_2,a_2,r_2,\cdots,s_n,a_n,r_n\)</span>.</li>
<li>计算所有的回报：<span class="math inline">\(u_t=\sum_{k=t}^T\gamma^{k-t} \cdot r_k, \quad
\forall t=1,\cdots,n\)</span>.</li>
<li>对辅助网络做反向传播，得到所有的梯度：<span class="math inline">\(\nabla_{\theta}f(s_t,a_t;\theta_{now}), \quad
\forall t=1,\cdots,n\)</span>.</li>
<li>用策略梯度上升更新参数：<span class="math inline">\(\theta_{new}
\leftarrow \theta_{now} + \beta \cdot \sum_{t=1}^{n}\gamma^{t-1} \cdot
u_t \cdot \nabla_{\theta}f(s_t,a_t;\theta_{now})\)</span>.</li>
</ol>
<p>上述算法标准的 REINFORCE, 效果不如使用基线的
REINFORCE。可以用基线改进上面描述的算法。REINFORCE
算法属于同策略(on-policy),不能使用经验回放。</p>
<h3 id="用-actor-critic-学习参数">用 Actor-Critic 学习参数</h3>
<p>Actor-critic 需要搭建一个价值网络<span class="math inline">\(q(s,a;w)\)</span>, 用于近似动作价值函数<span class="math inline">\(Q_{\pi}(s,a)\)</span>。价值网络的结构如图 10.12
所示。此外，还需要一个目标价值网络<span class="math inline">\(q(s,a;w^-)\)</span>,
网络结构相同，但是参数不同。</p>
<p><img src="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-2-%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG/1733282448343-5c0b3c82-4f80-452a-8908-2503c42f7ecc.png" srcset="/img/loading.gif" lazyload></p>
<p>在搭建好均值网络<span class="math inline">\(\mu\)</span>、方差对数网络<span class="math inline">\(\rho\)</span>、辅助网络<span class="math inline">\(f\)</span>、价值网络<span class="math inline">\(q\)</span>之后，我们用 SARSA
算法更新价值网络参数<span class="math inline">\(w\)</span>,
用近似策略梯度更新控制器参数<span class="math inline">\(\theta\)</span>。设当前参数为<span class="math inline">\(w_{now}\)</span>和<span class="math inline">\(\theta_{now}\)</span>。重复以下步骤更新价值网络参数、控制器参数，直到收敛：</p>
<ol type="1">
<li>实际观测到当前状态<span class="math inline">\(s_t\)</span>,用控制器算出均值<span class="math inline">\(\mu(s_t;\theta_{now})\)</span>和方差对数<span class="math inline">\(\rho(s_t;\theta_{now})\)</span>,然后随机抽样得到动作<span class="math inline">\(a_t\)</span>。智能体执行动作<span class="math inline">\(a_t\)</span>, 观测到奖励<span class="math inline">\(r_t\)</span>与新的状态<span class="math inline">\(s_{t+1}\)</span>。</li>
<li>计算均值<span class="math inline">\(\mu(s_{t+1};\theta_{now})\)</span>和方差对数<span class="math inline">\(\rho(s_{t+1};\theta_{now})\)</span>,然后随机抽样得到动作<span class="math inline">\(\tilde{a}_{t+1}\)</span>。这个动作只是假想动作，智能体不予执行。</li>
<li>用价值网络计算出：<span class="math inline">\(\hat{q}_t=q(s_t,a_t;w_{now})\)</span>.</li>
<li>用目标网络计算出：<span class="math inline">\(\hat{q}_{t+1}=q(s_{t+1},\tilde{a}_{t+1};w_{now}^-)\)</span>.</li>
<li>计算 TD 目标和 TD 误差：<span class="math inline">\(\hat{y}_t=r_t+\gamma \cdot
\hat{q}_{t+1}\)</span>,<span class="math inline">\(\delta_t=\hat{q}_t-\hat{y}_t\)</span>.</li>
<li>更新价值网络的参数：<span class="math inline">\(w_{new} \leftarrow
w_{now} - \alpha \cdot \delta_t \cdot \nabla_w
q(s_t,a_t;w_{now})\)</span>.</li>
<li>更新策略网络参数参数：<span class="math inline">\(\theta_{new}
\leftarrow \theta_{now} + \beta \cdot \hat{q}_t \cdot
\nabla_{\theta}f(s_t,a_t;\theta_{now})\)</span>.</li>
<li>更新目标网络参数：<span class="math inline">\(w_{new}^- \leftarrow
\tau \cdot w_{new} + (1-\tau) \cdot w_{now}^-\)</span>.</li>
</ol>
<p>算法中的<span class="math inline">\(\alpha\)</span>、<span class="math inline">\(\beta\)</span>、<span class="math inline">\(\tau\)</span>都是超参数，需要手动调整。上述算法是标准的
actor-critic. 效果不如advantage actor-critic
(A2C)。可以用A2C改进上述方法。</p>
<h1 id="总结">总结</h1>
<p>离散控制问题的动作空间<span class="math inline">\(A\)</span>是个有限的离散集，连续控制问题的动作空间<span class="math inline">\(A\)</span>是个连续集。如果想将 DQN
等离散控制方法应用到连续控制问题，可以对连续动作空间做离散化，但这只适用于自由度较小的问题。</p>
<p>可以用确定策略网络<span class="math inline">\(a=\mu(s;\theta)\)</span>做连续控制。网络的输入是状态<span class="math inline">\(s\)</span>, 输出是动作<span class="math inline">\(a\)</span>,<span class="math inline">\(a\)</span>是向量，大小等于问题的自由度。</p>
<p>确定策略梯度 (DPG) 借助价值网络<span class="math inline">\(q(s,a;w)\)</span>训练确定策略网络。DPG
属于异策略，用行为策略收集经验，做经验回放更新策略网络和价值网络。</p>
<p>DPG 与 DQN 有很多相似之处，而且它们的训练都存在高估等问题。TD3
使用几种技巧改进 DPG: 截断双 Q
学习、往动作中加噪声、降低更新策略网络和目标网络的频率。</p>
<p>可以用随机高斯策略做连续控制。用两个神经网络分别近似高斯分布的均值和方差对数，并用策略梯度更新两个神经网络的参数。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  
    <span>></span>
    
  <a href="/categories/AI/RL/" class="category-chain-item">RL</a>
  
  
    <span>></span>
    
  <a href="/categories/AI/RL/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/" class="category-chain-item">DRL-王树森</a>
  
  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/RL/" class="print-no-link">#RL</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>7.2 确定策略梯度 (DPG)</div>
      <div>http://binbo-zappy.github.io/2024/12/04/DRL-王树森/7-2-确定策略梯度-DPG/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Binbo</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年12月4日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/8-1-%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AB%98%E7%BA%A7%E6%8A%80%E5%B7%A7-%E7%BD%AE%E4%BF%A1%E5%9F%9F%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96-TRPO/" title="8.1 策略学习的高级技巧：置信域策略优化 (TRPO)">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">8.1 策略学习的高级技巧：置信域策略优化 (TRPO)</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/12/04/DRL-%E7%8E%8B%E6%A0%91%E6%A3%AE/7-1-%E8%BF%9E%E7%BB%AD%E6%8E%A7%E5%88%B6/" title="7.1 连续控制">
                        <span class="hidden-mobile">7.1 连续控制</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
